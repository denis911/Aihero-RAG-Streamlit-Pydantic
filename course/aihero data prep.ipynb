{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1cac94-1f14-44f9-a8bc-1c359b395889",
   "metadata": {},
   "source": [
    "## Download data and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17d7bfa-3616-4e22-b579-35b48511c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ddd76b1-7566-4643-b71c-495a3e5f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download the repository as a zip file. GitHub provides a convenient URL format for this:\n",
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "# NB!! This code downloads the file to memory, not to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abb396e-1759-4995-8203-bc5e4fefbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937e3e28-5a9c-4a25-b2de-eda7c108e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Donâ€™t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fe6486-4ce8-4346-9963-2b0e4f12faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Implementation\n",
    "# Let's now put everything together into a reusable function:\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ba8192-69c9-4667-9a4c-ae558620fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# We can now use this function for different repositories:\n",
    "    \n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fa09c1-64d1-4b01-8308-c1aaf4a81345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'bfafa427b3',\n",
       " 'question': 'Course: What are the prerequisites for this course?',\n",
       " 'sort_order': 2,\n",
       " 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).',\n",
       " 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5da3db9d-c3d6-41f6-8923-11f8e21435a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21712"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evidently_docs[45]['content']) # 21712 \n",
    "# his is too long - we need to apply chunking say for 2k symbols here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ee661-7503-4386-9eab-81c8ef6377f0",
   "metadata": {},
   "source": [
    "## Observations during data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93ae96-c133-4535-9fd3-8da19cb9246e",
   "metadata": {},
   "source": [
    "Data Processing Considerations\n",
    "\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "\n",
    "Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "\n",
    "Performance: AI models work better with shorter text segments\n",
    "\n",
    "Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques below\n",
    "\n",
    "TODO\n",
    "\n",
    "-- Create a new uv project in the project directory\n",
    "\n",
    "-- Select a GitHub repo with documentation (preferably with .md files) - I can look for network security repos for example \n",
    "\n",
    "-- Download the data from there using the techniques we've learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dd59df2-ecd7-4c24-a502-41ac731ab25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the document above at index 45 looks like:\n",
    "\n",
    "# {'title': 'LLM regression testing',\n",
    "#  'description': 'How to run regression testing for LLM outputs.',\n",
    "#  'content': 'In this tutorial, you will learn...'\n",
    "# }\n",
    "\n",
    "# The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. \n",
    "# For example, for size of 2000 characters, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 2000..4000\n",
    "# Chunk 3: 4000..6000\n",
    "\n",
    "# And so on.\n",
    "\n",
    "# However, this approach has disadvantages:\n",
    "\n",
    "# Context loss: Important information might be split in the middle\n",
    "# Incomplete sentences: Chunks might end mid-sentence\n",
    "# Missing connections: Related information might end up in different chunks\n",
    "\n",
    "# That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 1000..3000\n",
    "# Chunk 3: 2000..4000\n",
    "# ...\n",
    "\n",
    "# This is better for AI because:\n",
    "\n",
    "# Continuity: Important information isn't lost at chunk boundaries\n",
    "# Context preservation: Related sentences stay together in at least one chunk\n",
    "# Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "# This approach is known as the \"sliding window\" method. This is how we implement it in Python:\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "evidently_overlapping_chunks_45 = sliding_window(evidently_docs[45]['content'], 2000, 1000)\n",
    "evidently_overlapping_chunks_45[:2] # please note chunks are indeed OVERLAPPING below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7712ffea-99fa-498c-832a-c47e83e30c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process all the documents in Evidently text dump:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db2e2a57-5b73-4a24-8ed4-a31edce38698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "575"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we use copy() and pop() operations:\n",
    "\n",
    "# doc.copy() creates a shallow copy of the document dictionary\n",
    "# doc_copy.pop('content') removes the 'content' key and returns its value\n",
    "# This way we preserve the original dictionary keys that we can use later in the chunks.\n",
    "\n",
    "# This way, we obtain 575 chunks from 95 documents.\n",
    "\n",
    "# We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1034564f-21c8-4cf6-8839-cf7167189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidently_chunks[:4]\n",
    "\n",
    "# There are some alternative approaches:\n",
    "\n",
    "# Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "# Advantages: More precise control over LLM input size\n",
    "# Disadvantages: Doesn't work well for documents with code\n",
    "# Paragraph splitting: Split by paragraphs\n",
    "# Section splitting: Split by sections\n",
    "# AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "# We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n",
    "\n",
    "# We will implement the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bd04f-0507-4436-98fa-978c04408ecd",
   "metadata": {},
   "source": [
    "## Chunks continued - Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c03428c4-a9bb-4efa-b081-2c34fb17a54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In this tutorial, you will learn how to perform regression testing for LLM outputs.',\n",
       " 'You can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting by paragraphs is relatively easy:\n",
    "\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "\n",
    "# We use \\n\\s*\\n regex pattern for splitting:\n",
    "\n",
    "# \\n matches a newline\n",
    "# \\s* matches zero or more whitespace characters\n",
    "# \\n matches another newline\n",
    "# So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
    "\n",
    "# This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n",
    "paragraphs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ffa31e-bfa1-4505-85a4-c843dbabaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can combine sliding window and paragraph splitting for more intelligent processing. We won't do it here, but it's a good exercise to try.\n",
    "\n",
    "# Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# # Heading 1\n",
    "# ## Heading 2  \n",
    "# ### Heading 3\n",
    "\n",
    "# What we can do is split by headers.\n",
    "\n",
    "# For that we will use regex too:\n",
    "\n",
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. \n",
    "# But in general, this is not a big problem for documentation.\n",
    "\n",
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "# sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33119566-8e4a-41fd-995e-cf8d98a5288f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. \n",
    "# This way, each section gets the same metadata (title, description) as the original document.\n",
    "\n",
    "# This was more intelligent processing, but we can go even further and use LLMs for that.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "832ef228-c811-413c-9984-d64fec8400e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx',\n",
       "  'section': '## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>'},\n",
       " {'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx',\n",
       "  'section': '## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b3ac9-9965-4ac5-9462-288ba1af53bb",
   "metadata": {},
   "source": [
    "## Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5e8b638-d6c7-4f73-b62d-e3f0090df74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "# This makes sense when:\n",
    "\n",
    "# Complex structure: Documents have complex, non-standard structure\n",
    "# Semantic coherence: You want chunks that are semantically meaningful\n",
    "# Custom logic: You need domain-specific splitting rules\n",
    "# Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "# This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "# Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "# You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "# You have complex, unstructured documents\n",
    "# Quality is more important than cost\n",
    "# You have the budget for LLM processing\n",
    "\n",
    "# Note: You can use any alternative LLM provider. One option is Groq, which is free with rate limits. You can replace the OpenAI library with the Groq library and it should work.\n",
    "\n",
    "# To continue, you need to get the API key from https://platform.openai.com/api-keys (assuming you have an account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf322203-9e0c-4f07-ba1b-ed5aee6cde44",
   "metadata": {},
   "source": [
    "Let's stop Jupyter and create an environment variable with your key:\n",
    "\n",
    "'''\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "'''\n",
    "\n",
    "Install the OpenAI SDK:\n",
    "\n",
    "'''\n",
    "uv add openai\n",
    "'''\n",
    "\n",
    "Then run jupyter notebook:\n",
    "\n",
    "'''\n",
    "uv run jupyter notebook\n",
    "'''\n",
    "\n",
    "It's cumbersome to set environment variables every time. I recommend using direnv, which works for Linux, Mac and Windows.\n",
    "\n",
    "Note: if you use direnv, don't forget to add .envrc to .gitignore.\n",
    "\n",
    "Warning: Never commit your API keys to git! Others can use your API key and you'll pay for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9b52f5-fe7c-4c60-8afd-ae57ae471d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Enter your OpenAI API key:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08d9ab5b-cf85-45c0-bd3b-6a3867abb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use OpenAI:\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "# This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results. \n",
    "# We will explain in more detail what this code does in the next lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db073911-134a-40b2-8942-5a861999cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a prompt:\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "# The prompt asks the LLM to:\n",
    "\n",
    "# Split the document logically (not just by length)\n",
    "# Make sections self-contained\n",
    "# Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ce2a34e-3e53-44db-9ed0-997d52cdc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function for intelligent chunking:\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0789621-74a4-4b36-8ed7-5a1bede6772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0f7ec152de4aa7b788ed12ffae7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# tqdm is a library that shows progress bars. It helps you track progress when processing a large number of documents.\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. \n",
    "# For most applications, you don't need intelligent chunking.\n",
    "# this particular chunking took 30 mins and cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820e8c0c-bf67-413f-b06d-cfbd43f3a604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evidently_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevidently_chunks\u001b[49m[:\u001b[32m2\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'evidently_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "# evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09abc4a7-4cef-46a6-9e53-972cb1dcf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Choose a Chunking Approach\n",
    "# You may wonder - which chunking should I use? The answer: start with the simplest one and gradually increase complexity. \n",
    "# Start with simple chunking with overlaps. We will later talk about evaluations. \n",
    "# You can use evaluations to make informed decisions about chunking strategies.\n",
    "\n",
    "# Our data is ready. Now we can index it â€“ insert it into a search engine and make it available for our (future) agent to use.\n",
    "\n",
    "# TODO:\n",
    "# For the project you selected, apply chunking\n",
    "# Experiment with simple chunking, paragraph chunking + sliding window, and section chunking\n",
    "# Which approach makes sense for your application? \n",
    "# Manually inspect the results and analyze what works best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970c75a-5b3c-4b3f-a4b8-ed723959d464",
   "metadata": {},
   "source": [
    "## Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82052e65-665c-4cef-b44e-006e29dd5afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
