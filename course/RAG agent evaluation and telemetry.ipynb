{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9664264-5776-42d2-a228-4d3c52b22a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Why evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d9dfa-dad8-4b04-bf3a-9fe75c9e7a4d",
   "metadata": {},
   "source": [
    "Before - in AI HERO DATA PREP LONG notebook we learned about function calling and created our first agent using Pydantic AI.\n",
    "\n",
    "But is this agent actually good? Today we will see how to answer this question.\n",
    "\n",
    "In particular, we will cover:\n",
    "\n",
    "-- Build a logging system to track agent interactions\n",
    "\n",
    "-- Create automated evaluation using AI as a judge\n",
    "\n",
    "-- Generate test data automatically\n",
    "\n",
    "-- Measure agent performance with metrics\n",
    "\n",
    "At the end we will have a thoroughly tested agent with performance metrics.\n",
    "\n",
    "In this lesson, we'll use the FAQ database with text search, but it's applicable for any other use case.\n",
    "\n",
    "This is going to be a long lesson, but an important one. Evaluation is critical for building reliable AI systems. Without proper evaluation, you can't tell if your changes improve or hurt performance. You can't compare different approaches. And you can't build confidence before deploying to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011f3c0-8612-4272-bc8c-3602ac5bee09",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c07bf-a436-4d6b-a3a5-4a175152bd27",
   "metadata": {},
   "source": [
    "The easiest thing we can do to evaluate an agent is interact with it. We ask something and look at the response. Does it make sense? For most cases, it should.\n",
    "\n",
    "This approach is called \"vibe check\" - we interact with it, and if we like the results, we go ahead and deploy it.\n",
    "\n",
    "If we don't like something, we go back and change things:\n",
    "\n",
    "-- Maybe our chunking method is not suitable? Maybe we need to have a bigger window size?\n",
    "\n",
    "-- Is our system prompt good? Maybe we need more precise instructions?\n",
    "\n",
    "Or we want to change something else\n",
    "\n",
    "And we iterate.\n",
    "\n",
    "It might be okay for the first MVP, but how can we make sure the result at the end is actually good?\n",
    "\n",
    "We need systematic evaluation. Manual testing doesn't scale - you can't manually test every possible input and scenario. With systematic evaluation, we can test hundreds or thousands of cases automatically.\n",
    "\n",
    "We also need to base our decisions on data. It will help us to\n",
    "\n",
    "-- Compare different approaches\n",
    "\n",
    "-- Track improvements\n",
    "\n",
    "-- Identify edge cases\n",
    "\n",
    "We can start collecting this data ourselves: start with vibe checking, but be smart about it. We don't just test it, but also record the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677424a2-ed46-4a9f-8648-8e0a56136ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data scrape function\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1831c019-957c-4dfd-911b-038409a4c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# download data as we did earlier:\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1373fe23-852b-497a-8ffd-f61775b64923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1a532390e00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a simple text search index for FAQ data:\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']] # we extract only DE = data engineering FAQ\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# <minsearch.minsearch.Index at 0x1ed6ba1bcb0>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913f0f76-5bf6-4221-930e-e56fa7572831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üîë Enter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee1f64c-9840-4ccf-8b72-18372921dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the agent we created yesterday:\n",
    "\n",
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0f3830-df0c-4c36-bb73-4f1f2adc6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's interact with the agent:\n",
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214d8d96-1e87-4692-b2fe-24cb30f0a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='To install Kafka in Python, you can use the following commands to set up the necessary dependencies:\\n\\n1. **Install `confluent-kafka`:**\\n   - Using pip:\\n     ```bash\\n     pip install confluent-kafka\\n     ```\\n   - Using conda:\\n     ```bash\\n     conda install conda-forge::python-confluent-kafka\\n     ```\\n\\n2. **Install `fastavro`:**\\n   ```bash\\n   pip install fastavro\\n   ```\\n\\nThese commands will help you set up the essential packages for working with Kafka in Python. If you encounter any issues, feel free to ask for more specific guidance!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110f775-4eb9-49e0-8db4-c55321a50365",
   "metadata": {},
   "source": [
    "Here's what we want to record:\n",
    "\n",
    "-- The system prompt that we used\n",
    "\n",
    "-- The model\n",
    "\n",
    "-- The user query\n",
    "\n",
    "-- The tools we use\n",
    "\n",
    "-- The responses and the back-and-forth interactions between the LLM and our tools\n",
    "\n",
    "-- The final response\n",
    "\n",
    "To make it simpler, we'll implement a simple logging system ourselves: we will just write logs to json files.\n",
    "\n",
    "You shouldn't use it in production. In practice, you will want to send these logs to some log collection system, or use specialized LLM evaluation tools like Evidently, LangWatch or Arize Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5845f0a-911e-4977-bcf1-458ab77f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract all this information from the agent and from the run results:\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "# This code extracts the key information from our agent:\n",
    "# -- the configuration (name, prompt, model)\n",
    "# -- available tools\n",
    "# -- complete message history (user input, tool calls, responses)\n",
    "# We also use ModelMessagesTypeAdapter.dump_python(messages) to convert internal message format into regular Python dictionaries. \n",
    "# This makes it easier to save it to JSON and process later.\n",
    "# We also add the source parameter. It tracks where the question came from. We start with \"user\" but later we'll use AI-generated queries. \n",
    "# Sometimes it may be important to tell them apart for analysis.\n",
    "# This code is generic so it will work with any Pydantic AI agent. If you use a different library, you'll need to adjust this code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5861b9-ff62-48f9-b5e2-0a2757acca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write these logs to a folder:\n",
    "\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# This code above:\n",
    "# Creates a logs directory (if not created previously)\n",
    "# Generates unique filenames with timestamp and random hex\n",
    "# Saves complete interaction logs as JSON files\n",
    "# Handles datetime serialization (using the serialized function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8576867-f31a-414a-9f55-c548a271eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on Windows?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Windows, follow these guidelines based on your version:\n",
      "\n",
      "### For Windows 10 Pro / 11 Pro Users:\n",
      "1. **Enable Hyper-V**: Docker can utilize Hyper-V as a backend. You can enable Hyper-V by following a tutorial such as [this one](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\n",
      "   \n",
      "2. **Download Docker Desktop**: Install Docker Desktop for Windows from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/).\n",
      "\n",
      "### For Windows 10 Home / 11 Home Users:\n",
      "1. **Use WSL2**: The Home edition does not support Hyper-V. Instead, Docker uses WSL2 (Windows Subsystem for Linux). You can install WSL2 by following instructions from [this guide](https://pureinfotech.com/install-wsl-windows-11/).\n",
      "\n",
      "2. **Update WSL2**: If you encounter the error \"WslRegisterDistribution failed with error: 0x800701bc\", make sure to update the WSL2 Linux Kernel by following guidelines provided at this [GitHub issue](https://github.com/microsoft/WSL/issues/5393).\n",
      "\n",
      "### Common Troubleshooting:\n",
      "- **Elevated Privileges**: If you run into an error about needing elevated privileges, make sure you are running the Docker client with administrative permissions.\n",
      "- **Initial Setup**: Make sure you have the latest version of Docker installed. If Docker seems stuck or does not start, uninstalling and reinstalling might help.\n",
      "- **Switching Containers**: If Docker is stuck on starting, try switching the backend by right-clicking the Docker icon in the system tray and selecting an appropriate backend (Windows or Linux).\n",
      "\n",
      "By following these instructions, you should be able to successfully set up and run Docker on your Windows machine. If you have any specific issues, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_20251015_161727_414680.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now we can interact with it and do some vibe checking:\n",
    "    \n",
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# This creates a simple interactive loop where:\n",
    "# -- User enters a question\n",
    "# -- Agent processes it and responds\n",
    "# -- Complete interaction is logged to a file\n",
    "\n",
    "# Try these questions:\n",
    "# how do I use docker on windows?\n",
    "# can I join late and get a certificate?\n",
    "# what do I need to do for the certificate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899b9eb-87d4-4f59-9f5f-7d6085fcae13",
   "metadata": {},
   "source": [
    "## Adding References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2e366d-66bf-4d2d-87dc-b901a0e2f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When interacting with the agent, I noticed one thing: it doesn't include the reference to the original documents.\n",
    "# Let's fix it by adjusting the prompt:\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "correct link looks like this - without << /faq-main >> part :\n",
    "<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# This is the output I now get for the question \"can I join late and get a certificate?\":\n",
    "\n",
    "# Yes, you can join the course late and still be eligible for a certificate, as long as you complete the required peer-reviewed \n",
    "# capstone projects on time. You do not need to complete the homework assignments if you join late, which allows for flexibility in participation.\n",
    "# However, please note that certificates are only awarded to those who finish the course with a ‚Äúlive‚Äù cohort; \n",
    "# they are not available for those who choose the self-paced mode. \n",
    "# This is because peer-reviewing capstone projects is a requirement that can only be done while the course is active......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a830263-fc38-4c89-856c-8ee6d4669cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on linux?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Linux, you may need to follow these general steps:\n",
      "\n",
      "1. **Installation**: Depending on your Linux distribution, you can install Docker using different methods. For example, on Ubuntu, you can use the snap command:\n",
      "\n",
      "   ```bash\n",
      "   sudo snap install docker\n",
      "   ```\n",
      "\n",
      "   This command installs Docker via snap packages, which might be available on certain versions of Ubuntu. [Learn more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/036_1b727dde32_docker-docker-not-installable-on-ubuntu.md).\n",
      "\n",
      "2. **Extra Hosts Configuration**: If you're using Docker Compose and face issues resolving `host.docker.internal`, you might need to configure it in your `docker-compose.yml` file like so:\n",
      "\n",
      "   ```yaml\n",
      "   kestra:\n",
      "     image: kestra/kestra:latest\n",
      "     extra_hosts:\n",
      "       - \"host.docker.internal:host-gateway\"\n",
      "   ```\n",
      "\n",
      "   This setup allows your container to access host services correctly. You can check more details on this configuration. [Read more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-2/015_48b99a69ff_fix-add-extra_hosts-for-hostdockerinternal-on-linu.md).\n",
      "\n",
      "3. **Using Docker Commands**: After installing Docker, you can start using Docker commands in the terminal. Common commands include:\n",
      "   - `docker run`: To run a container.\n",
      "   - `docker ps`: To list running containers.\n",
      "   - `docker images`: To list available images.\n",
      "\n",
      "4. **Docker Compose**: If you need to orchestrate multi-container applications, Docker Compose is a useful tool. Ensure it's installed and configured properly. If after installation you have trouble using it, ensure that you might need to rename the downloaded Docker Compose binary to `docker-compose` for easy access. Here's how you can rename it:\n",
      "\n",
      "   ```bash\n",
      "   mv docker-compose-linux-x86_64 docker-compose\n",
      "   ```\n",
      "\n",
      "   This resolves issues where the command may not be recognized. [Find more about Docker Compose here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md).\n",
      "\n",
      "5. **Starting Docker**: Make sure that the Docker service is running. You can start Docker with:\n",
      "\n",
      "   ```bash\n",
      "   sudo systemctl start docker\n",
      "   ```\n",
      "\n",
      "By following these steps, you should be able to effectively use Docker on a Linux system. If you have any specific questions, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_v2_20251015_161845_62dcb0.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# NB - links were broken in original code - I checked generated links and model simply merge 2 http adress parts instead of replacing :\n",
    "# Wrong link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# correct link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# as I manually deleted /faq-main part ...\n",
    "# I FIXED PROMPT ABOVE AND NOW IT WORKS WELL !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeccb47-c8c9-4d27-a72e-361e4b817b0d",
   "metadata": {},
   "source": [
    "## LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90b9a485-8754-46f5-9b0b-bf2051d8f37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928c7ea-eeea-4f16-b10f-a327b6d6a665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
