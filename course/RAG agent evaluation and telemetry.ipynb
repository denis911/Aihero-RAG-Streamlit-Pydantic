{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9664264-5776-42d2-a228-4d3c52b22a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Why evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d9dfa-dad8-4b04-bf3a-9fe75c9e7a4d",
   "metadata": {},
   "source": [
    "Before - in AI HERO DATA PREP LONG notebook we learned about function calling and created our first agent using Pydantic AI.\n",
    "\n",
    "But is this agent actually good? Today we will see how to answer this question.\n",
    "\n",
    "In particular, we will cover:\n",
    "\n",
    "-- Build a logging system to track agent interactions\n",
    "\n",
    "-- Create automated evaluation using AI as a judge\n",
    "\n",
    "-- Generate test data automatically\n",
    "\n",
    "-- Measure agent performance with metrics\n",
    "\n",
    "At the end we will have a thoroughly tested agent with performance metrics.\n",
    "\n",
    "In this lesson, we'll use the FAQ database with text search, but it's applicable for any other use case.\n",
    "\n",
    "This is going to be a long lesson, but an important one. Evaluation is critical for building reliable AI systems. Without proper evaluation, you can't tell if your changes improve or hurt performance. You can't compare different approaches. And you can't build confidence before deploying to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011f3c0-8612-4272-bc8c-3602ac5bee09",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c07bf-a436-4d6b-a3a5-4a175152bd27",
   "metadata": {},
   "source": [
    "The easiest thing we can do to evaluate an agent is interact with it. We ask something and look at the response. Does it make sense? For most cases, it should.\n",
    "\n",
    "This approach is called \"vibe check\" - we interact with it, and if we like the results, we go ahead and deploy it.\n",
    "\n",
    "If we don't like something, we go back and change things:\n",
    "\n",
    "-- Maybe our chunking method is not suitable? Maybe we need to have a bigger window size?\n",
    "\n",
    "-- Is our system prompt good? Maybe we need more precise instructions?\n",
    "\n",
    "Or we want to change something else\n",
    "\n",
    "And we iterate.\n",
    "\n",
    "It might be okay for the first MVP, but how can we make sure the result at the end is actually good?\n",
    "\n",
    "We need systematic evaluation. Manual testing doesn't scale - you can't manually test every possible input and scenario. With systematic evaluation, we can test hundreds or thousands of cases automatically.\n",
    "\n",
    "We also need to base our decisions on data. It will help us to\n",
    "\n",
    "-- Compare different approaches\n",
    "\n",
    "-- Track improvements\n",
    "\n",
    "-- Identify edge cases\n",
    "\n",
    "We can start collecting this data ourselves: start with vibe checking, but be smart about it. We don't just test it, but also record the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677424a2-ed46-4a9f-8648-8e0a56136ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data scrape function\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1831c019-957c-4dfd-911b-038409a4c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# download data as we did earlier:\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1373fe23-852b-497a-8ffd-f61775b64923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1a532390e00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a simple text search index for FAQ data:\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']] # we extract only DE = data engineering FAQ\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# <minsearch.minsearch.Index at 0x1ed6ba1bcb0>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913f0f76-5bf6-4221-930e-e56fa7572831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üîë Enter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee1f64c-9840-4ccf-8b72-18372921dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the agent we created yesterday:\n",
    "\n",
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0f3830-df0c-4c36-bb73-4f1f2adc6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's interact with the agent:\n",
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214d8d96-1e87-4692-b2fe-24cb30f0a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='To install Kafka in Python, you can use the following commands to set up the necessary dependencies:\\n\\n1. **Install `confluent-kafka`:**\\n   - Using pip:\\n     ```bash\\n     pip install confluent-kafka\\n     ```\\n   - Using conda:\\n     ```bash\\n     conda install conda-forge::python-confluent-kafka\\n     ```\\n\\n2. **Install `fastavro`:**\\n   ```bash\\n   pip install fastavro\\n   ```\\n\\nThese commands will help you set up the essential packages for working with Kafka in Python. If you encounter any issues, feel free to ask for more specific guidance!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110f775-4eb9-49e0-8db4-c55321a50365",
   "metadata": {},
   "source": [
    "Here's what we want to record:\n",
    "\n",
    "-- The system prompt that we used\n",
    "\n",
    "-- The model\n",
    "\n",
    "-- The user query\n",
    "\n",
    "-- The tools we use\n",
    "\n",
    "-- The responses and the back-and-forth interactions between the LLM and our tools\n",
    "\n",
    "-- The final response\n",
    "\n",
    "To make it simpler, we'll implement a simple logging system ourselves: we will just write logs to json files.\n",
    "\n",
    "You shouldn't use it in production. In practice, you will want to send these logs to some log collection system, or use specialized LLM evaluation tools like Evidently, LangWatch or Arize Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5845f0a-911e-4977-bcf1-458ab77f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract all this information from the agent and from the run results:\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "# This code extracts the key information from our agent:\n",
    "# -- the configuration (name, prompt, model)\n",
    "# -- available tools\n",
    "# -- complete message history (user input, tool calls, responses)\n",
    "# We also use ModelMessagesTypeAdapter.dump_python(messages) to convert internal message format into regular Python dictionaries. \n",
    "# This makes it easier to save it to JSON and process later.\n",
    "# We also add the source parameter. It tracks where the question came from. We start with \"user\" but later we'll use AI-generated queries. \n",
    "# Sometimes it may be important to tell them apart for analysis.\n",
    "# This code is generic so it will work with any Pydantic AI agent. If you use a different library, you'll need to adjust this code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5861b9-ff62-48f9-b5e2-0a2757acca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write these logs to a folder:\n",
    "\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# This code above:\n",
    "# Creates a logs directory (if not created previously)\n",
    "# Generates unique filenames with timestamp and random hex\n",
    "# Saves complete interaction logs as JSON files\n",
    "# Handles datetime serialization (using the serialized function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8576867-f31a-414a-9f55-c548a271eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on Windows?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Windows, follow these guidelines based on your version:\n",
      "\n",
      "### For Windows 10 Pro / 11 Pro Users:\n",
      "1. **Enable Hyper-V**: Docker can utilize Hyper-V as a backend. You can enable Hyper-V by following a tutorial such as [this one](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\n",
      "   \n",
      "2. **Download Docker Desktop**: Install Docker Desktop for Windows from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/).\n",
      "\n",
      "### For Windows 10 Home / 11 Home Users:\n",
      "1. **Use WSL2**: The Home edition does not support Hyper-V. Instead, Docker uses WSL2 (Windows Subsystem for Linux). You can install WSL2 by following instructions from [this guide](https://pureinfotech.com/install-wsl-windows-11/).\n",
      "\n",
      "2. **Update WSL2**: If you encounter the error \"WslRegisterDistribution failed with error: 0x800701bc\", make sure to update the WSL2 Linux Kernel by following guidelines provided at this [GitHub issue](https://github.com/microsoft/WSL/issues/5393).\n",
      "\n",
      "### Common Troubleshooting:\n",
      "- **Elevated Privileges**: If you run into an error about needing elevated privileges, make sure you are running the Docker client with administrative permissions.\n",
      "- **Initial Setup**: Make sure you have the latest version of Docker installed. If Docker seems stuck or does not start, uninstalling and reinstalling might help.\n",
      "- **Switching Containers**: If Docker is stuck on starting, try switching the backend by right-clicking the Docker icon in the system tray and selecting an appropriate backend (Windows or Linux).\n",
      "\n",
      "By following these instructions, you should be able to successfully set up and run Docker on your Windows machine. If you have any specific issues, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_20251015_161727_414680.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now we can interact with it and do some vibe checking:\n",
    "    \n",
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# This creates a simple interactive loop where:\n",
    "# -- User enters a question\n",
    "# -- Agent processes it and responds\n",
    "# -- Complete interaction is logged to a file\n",
    "\n",
    "# Try these questions:\n",
    "# how do I use docker on windows?\n",
    "# can I join late and get a certificate?\n",
    "# what do I need to do for the certificate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899b9eb-87d4-4f59-9f5f-7d6085fcae13",
   "metadata": {},
   "source": [
    "## Adding References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2e366d-66bf-4d2d-87dc-b901a0e2f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When interacting with the agent, I noticed one thing: it doesn't include the reference to the original documents.\n",
    "# Let's fix it by adjusting the prompt:\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "correct link looks like this - without << /faq-main >> part :\n",
    "<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# This is the output I now get for the question \"can I join late and get a certificate?\":\n",
    "\n",
    "# Yes, you can join the course late and still be eligible for a certificate, as long as you complete the required peer-reviewed \n",
    "# capstone projects on time. You do not need to complete the homework assignments if you join late, which allows for flexibility in participation.\n",
    "# However, please note that certificates are only awarded to those who finish the course with a ‚Äúlive‚Äù cohort; \n",
    "# they are not available for those who choose the self-paced mode. \n",
    "# This is because peer-reviewing capstone projects is a requirement that can only be done while the course is active......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a830263-fc38-4c89-856c-8ee6d4669cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on linux?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Linux, you may need to follow these general steps:\n",
      "\n",
      "1. **Installation**: Depending on your Linux distribution, you can install Docker using different methods. For example, on Ubuntu, you can use the snap command:\n",
      "\n",
      "   ```bash\n",
      "   sudo snap install docker\n",
      "   ```\n",
      "\n",
      "   This command installs Docker via snap packages, which might be available on certain versions of Ubuntu. [Learn more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/036_1b727dde32_docker-docker-not-installable-on-ubuntu.md).\n",
      "\n",
      "2. **Extra Hosts Configuration**: If you're using Docker Compose and face issues resolving `host.docker.internal`, you might need to configure it in your `docker-compose.yml` file like so:\n",
      "\n",
      "   ```yaml\n",
      "   kestra:\n",
      "     image: kestra/kestra:latest\n",
      "     extra_hosts:\n",
      "       - \"host.docker.internal:host-gateway\"\n",
      "   ```\n",
      "\n",
      "   This setup allows your container to access host services correctly. You can check more details on this configuration. [Read more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-2/015_48b99a69ff_fix-add-extra_hosts-for-hostdockerinternal-on-linu.md).\n",
      "\n",
      "3. **Using Docker Commands**: After installing Docker, you can start using Docker commands in the terminal. Common commands include:\n",
      "   - `docker run`: To run a container.\n",
      "   - `docker ps`: To list running containers.\n",
      "   - `docker images`: To list available images.\n",
      "\n",
      "4. **Docker Compose**: If you need to orchestrate multi-container applications, Docker Compose is a useful tool. Ensure it's installed and configured properly. If after installation you have trouble using it, ensure that you might need to rename the downloaded Docker Compose binary to `docker-compose` for easy access. Here's how you can rename it:\n",
      "\n",
      "   ```bash\n",
      "   mv docker-compose-linux-x86_64 docker-compose\n",
      "   ```\n",
      "\n",
      "   This resolves issues where the command may not be recognized. [Find more about Docker Compose here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md).\n",
      "\n",
      "5. **Starting Docker**: Make sure that the Docker service is running. You can start Docker with:\n",
      "\n",
      "   ```bash\n",
      "   sudo systemctl start docker\n",
      "   ```\n",
      "\n",
      "By following these steps, you should be able to effectively use Docker on a Linux system. If you have any specific questions, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_v2_20251015_161845_62dcb0.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# NB - links were broken in original code - I checked generated links and model simply merge 2 http adress parts instead of replacing :\n",
    "# Wrong link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# correct link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# as I manually deleted /faq-main part ...\n",
    "# I FIXED PROMPT ABOVE AND NOW IT WORKS WELL !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeccb47-c8c9-4d27-a72e-361e4b817b0d",
   "metadata": {},
   "source": [
    "## LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b9a485-8754-46f5-9b0b-bf2051d8f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can ask your colleagues to also do a \"vibe check\", but make sure you record the data. \n",
    "# Often collecting 10-20 examples and manually inspecting them is enough to understand how your model is doing.\n",
    "# Don't be afraid of putting manual work into evaluation. Manual evaluation will help you understand edge cases, \n",
    "# learn what good responses look like and think of evaluation criteria for automated checks later.\n",
    "# For example, I manually inspected the output and noticed that references are missing. So we will later add it as one of the checks.\n",
    "# So, in our case, we can have the following checks:\n",
    "# Does the agent follow the instructions?\n",
    "# Given the question, does the answer make sense?\n",
    "# Does it include references?\n",
    "# Did the agent use the available tools?\n",
    "# We don't have to evaluate this manually. Instead, we can delegate this to AI. This technique is called \"LLM as a Judge\".\n",
    "# The idea is simple: we use one LLM to evaluate the outputs of another LLM. This works because LLMs are good at following detailed evaluation criteria.\n",
    "    \n",
    "# Our system prompt for the judge (we'll call it \"evaluation agent\" because it sounds cooler) can look like that:\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d928c7ea-eeea-4f16-b10f-a327b6d6a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we expect a very well defined structure of the response, we can use structured output.\n",
    "# We can define a Pydantic class with the expected response structure, and the LLM will produce output that matches this schema exactly.\n",
    "    \n",
    "# This is how we do it:\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "\n",
    "# This code defines the structure we expect from our evaluation:\n",
    "# Each check has a name, justification, and pass/fail result\n",
    "# The overall evaluation includes a list of checks and a summary\n",
    "# Note that justification comes before check_pass. This makes \n",
    "# the LLM reason about the answer before giving the final judgment, \n",
    "# which typically leads to better evaluation quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58e8fe1-34ee-442b-9d3e-82482a50727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Pydantic AI in order to make the output follow the specified class, we use the parameter output_type:\n",
    "    \n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14079151-f6de-4756-a144-7f7b983b3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually it's a good idea to evaluate the results of one model (in our case, \"gpt-4o-mini\") \n",
    "# with another model (e.g. \"gpt-5-nano\"). A different model can catch mistakes, reduce self-bias, \n",
    "# and give a second opinion. This makes evaluations more reliable.\n",
    "# We have the instructions, and we have the agent. In order to run the agent, it needs input. \n",
    "    \n",
    "# We'll start with a template:\n",
    "\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "# We use XML markup because it's easier and more clear for LLMs to understand the input. \n",
    "# XML tags help the model see the structure and boundaries of different sections in the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812b1c26-8ac1-4c6d-b7e7-8bc2deddb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill it in. First, define a helper function for loading JSON log files:\n",
    "\n",
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "# We also add the filename in the result - it'll help us with tracking later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9fb925-6552-4464-9160-153921afde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it:\n",
    "# log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "# here I copied full path and replaced Windows backslashes \\ to forward slashes /\n",
    "log_record = load_log_file('C:/tmp/aihero/course/logs/faq_agent_20251015_155744_df13b1.json')\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddbb8b98-cd22-4d6a-aab7-8fd86596c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated a search tool call to fetch course-material relevant Docker-on-Linux guidance; awaiting results to craft a complete, sourced answer.\n",
      "check_name='instructions_follow' justification=\"The agent will follow the user's instruction to search course materials before answering (per <INSTRUCTIONS>). The agent hasn't yet, but will trigger a search tool in this response as required.\" check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed behavior observed. The response will rely on course materials and common Docker usage; nothing problematic. ' check_pass=True\n",
      "check_name='answer_relevant' justification='The answer will address Docker use on Linux. ' check_pass=True\n",
      "check_name='answer_clear' justification='The final answer will present steps clearly. ' check_pass=True\n",
      "check_name='answer_citations' justification='The answer will cite course material results from search; or at least reference them. ' check_pass=True\n",
      "check_name='completeness' justification='The answer will cover installation, starting service, permissions, running container, docker-compose, etc. ' check_pass=True\n",
      "check_name='tool_call_search' justification='We will call a search tool as required. ' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "# The user input is ready and we can test it!\n",
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)\n",
    "\n",
    "# This code:\n",
    "# Loads a saved interaction log\n",
    "# Extracts the key components (instructions, question, answer, full log)\n",
    "# Formats them into the evaluation prompt\n",
    "# Runs the evaluation agent\n",
    "# Prints the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "817018ac-7ec2-48ff-af17-8be761f53f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're putting the entire conversation log into the prompt, which is not really necessary. \n",
    "# We can reduce it to make it less verbose.\n",
    "# For example, like that:\n",
    "\n",
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n",
    "\n",
    "# We make it simpler:\n",
    "# remove timestamps and IDs that aren't needed for evaluation\n",
    "# replace actual search results with a placeholder\n",
    "# keep only the essential structure\n",
    "# This is helpful because it reduces the number of tokens we send to the evaluation model, \n",
    "# which lowers the costs and speeds up evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5eab01e7-2cbd-45ff-ba09-2d7a5d55d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put everything together:\n",
    "\n",
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "# log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "# here I copied full path and replaced Windows backslashes \\ to forward slashes /\n",
    "log_record = load_log_file('C:/tmp/aihero/course/logs/faq_agent_20251015_155744_df13b1.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)\n",
    "\n",
    "# We know how to log our data and how to run evals on our logs.\n",
    "# Great. But how do we get more data to get a better understanding of the performance of our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a16893a-d098-465c-bc2e-2da061c9bef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', justification='The user instruction required using a search tool to fetch course materials before answering; a text_search tool call is present in the log, indicating this was done.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='No disallowed actions or content were involved; the answer stayed within typical Docker usage guidance.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The answer directly explains how to install, start, configure, and use Docker on Linux, addressing the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The steps are organized and include concrete commands, making the guidance easy to follow.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='No citations were included, but citations are not strictly required for this general setup guide.', check_pass=True), EvaluationCheck(check_name='completeness', justification='Covers installation, service management, user permissions, basic commands, running containers, Compose, and host.docker.internal note.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The log shows a text_search tool invocation for this topic.', check_pass=True)], summary='All checklist criteria are satisfied. The agent followed the search instruction, addressed the Docker-on-Linux setup comprehensively, and avoided issues. Citations were not required for this general guidance.')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e9eb4-8172-4d8b-ab99-6dbab87d4d34",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662035b2-f456-47de-a986-33786eb44b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "# We can ask AI to help. What if we used it for generating more questions? Let's do that.\n",
    "# We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record. \n",
    "# We use this question as input to our agent and log the answers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
