{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9664264-5776-42d2-a228-4d3c52b22a4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Why evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d9dfa-dad8-4b04-bf3a-9fe75c9e7a4d",
   "metadata": {},
   "source": [
    "Before - in AI HERO DATA PREP LONG notebook we learned about function calling and created our first agent using Pydantic AI.\n",
    "\n",
    "But is this agent actually good? Today we will see how to answer this question.\n",
    "\n",
    "In particular, we will cover:\n",
    "\n",
    "-- Build a logging system to track agent interactions\n",
    "\n",
    "-- Create automated evaluation using AI as a judge\n",
    "\n",
    "-- Generate test data automatically\n",
    "\n",
    "-- Measure agent performance with metrics\n",
    "\n",
    "At the end we will have a thoroughly tested agent with performance metrics.\n",
    "\n",
    "In this lesson, we'll use the FAQ database with text search, but it's applicable for any other use case.\n",
    "\n",
    "This is going to be a long lesson, but an important one. Evaluation is critical for building reliable AI systems. Without proper evaluation, you can't tell if your changes improve or hurt performance. You can't compare different approaches. And you can't build confidence before deploying to users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011f3c0-8612-4272-bc8c-3602ac5bee09",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c07bf-a436-4d6b-a3a5-4a175152bd27",
   "metadata": {},
   "source": [
    "The easiest thing we can do to evaluate an agent is interact with it. We ask something and look at the response. Does it make sense? For most cases, it should.\n",
    "\n",
    "This approach is called \"vibe check\" - we interact with it, and if we like the results, we go ahead and deploy it.\n",
    "\n",
    "If we don't like something, we go back and change things:\n",
    "\n",
    "-- Maybe our chunking method is not suitable? Maybe we need to have a bigger window size?\n",
    "\n",
    "-- Is our system prompt good? Maybe we need more precise instructions?\n",
    "\n",
    "Or we want to change something else\n",
    "\n",
    "And we iterate.\n",
    "\n",
    "It might be okay for the first MVP, but how can we make sure the result at the end is actually good?\n",
    "\n",
    "We need systematic evaluation. Manual testing doesn't scale - you can't manually test every possible input and scenario. With systematic evaluation, we can test hundreds or thousands of cases automatically.\n",
    "\n",
    "We also need to base our decisions on data. It will help us to\n",
    "\n",
    "-- Compare different approaches\n",
    "\n",
    "-- Track improvements\n",
    "\n",
    "-- Identify edge cases\n",
    "\n",
    "We can start collecting this data ourselves: start with vibe checking, but be smart about it. We don't just test it, but also record the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677424a2-ed46-4a9f-8648-8e0a56136ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full data scrape function\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1831c019-957c-4dfd-911b-038409a4c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# download data as we did earlier:\n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1373fe23-852b-497a-8ffd-f61775b64923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1a532390e00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a simple text search index for FAQ data:\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "# dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']] # we extract only DE = data engineering FAQ\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n",
    "\n",
    "# <minsearch.minsearch.Index at 0x1ed6ba1bcb0>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913f0f76-5bf6-4221-930e-e56fa7572831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üîë Enter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee1f64c-9840-4ccf-8b72-18372921dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the agent we created yesterday:\n",
    "\n",
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0f3830-df0c-4c36-bb73-4f1f2adc6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's interact with the agent:\n",
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214d8d96-1e87-4692-b2fe-24cb30f0a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='To install Kafka in Python, you can use the following commands to set up the necessary dependencies:\\n\\n1. **Install `confluent-kafka`:**\\n   - Using pip:\\n     ```bash\\n     pip install confluent-kafka\\n     ```\\n   - Using conda:\\n     ```bash\\n     conda install conda-forge::python-confluent-kafka\\n     ```\\n\\n2. **Install `fastavro`:**\\n   ```bash\\n   pip install fastavro\\n   ```\\n\\nThese commands will help you set up the essential packages for working with Kafka in Python. If you encounter any issues, feel free to ask for more specific guidance!')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110f775-4eb9-49e0-8db4-c55321a50365",
   "metadata": {},
   "source": [
    "Here's what we want to record:\n",
    "\n",
    "-- The system prompt that we used\n",
    "\n",
    "-- The model\n",
    "\n",
    "-- The user query\n",
    "\n",
    "-- The tools we use\n",
    "\n",
    "-- The responses and the back-and-forth interactions between the LLM and our tools\n",
    "\n",
    "-- The final response\n",
    "\n",
    "To make it simpler, we'll implement a simple logging system ourselves: we will just write logs to json files.\n",
    "\n",
    "You shouldn't use it in production. In practice, you will want to send these logs to some log collection system, or use specialized LLM evaluation tools like Evidently, LangWatch or Arize Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5845f0a-911e-4977-bcf1-458ab77f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's extract all this information from the agent and from the run results:\n",
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }\n",
    "\n",
    "# This code extracts the key information from our agent:\n",
    "# -- the configuration (name, prompt, model)\n",
    "# -- available tools\n",
    "# -- complete message history (user input, tool calls, responses)\n",
    "# We also use ModelMessagesTypeAdapter.dump_python(messages) to convert internal message format into regular Python dictionaries. \n",
    "# This makes it easier to save it to JSON and process later.\n",
    "# We also add the source parameter. It tracks where the question came from. We start with \"user\" but later we'll use AI-generated queries. \n",
    "# Sometimes it may be important to tell them apart for analysis.\n",
    "# This code is generic so it will work with any Pydantic AI agent. If you use a different library, you'll need to adjust this code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b5861b9-ff62-48f9-b5e2-0a2757acca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write these logs to a folder:\n",
    "\n",
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# This code above:\n",
    "# Creates a logs directory (if not created previously)\n",
    "# Generates unique filenames with timestamp and random hex\n",
    "# Saves complete interaction logs as JSON files\n",
    "# Handles datetime serialization (using the serialized function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8576867-f31a-414a-9f55-c548a271eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on Windows?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Windows, follow these guidelines based on your version:\n",
      "\n",
      "### For Windows 10 Pro / 11 Pro Users:\n",
      "1. **Enable Hyper-V**: Docker can utilize Hyper-V as a backend. You can enable Hyper-V by following a tutorial such as [this one](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\n",
      "   \n",
      "2. **Download Docker Desktop**: Install Docker Desktop for Windows from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/).\n",
      "\n",
      "### For Windows 10 Home / 11 Home Users:\n",
      "1. **Use WSL2**: The Home edition does not support Hyper-V. Instead, Docker uses WSL2 (Windows Subsystem for Linux). You can install WSL2 by following instructions from [this guide](https://pureinfotech.com/install-wsl-windows-11/).\n",
      "\n",
      "2. **Update WSL2**: If you encounter the error \"WslRegisterDistribution failed with error: 0x800701bc\", make sure to update the WSL2 Linux Kernel by following guidelines provided at this [GitHub issue](https://github.com/microsoft/WSL/issues/5393).\n",
      "\n",
      "### Common Troubleshooting:\n",
      "- **Elevated Privileges**: If you run into an error about needing elevated privileges, make sure you are running the Docker client with administrative permissions.\n",
      "- **Initial Setup**: Make sure you have the latest version of Docker installed. If Docker seems stuck or does not start, uninstalling and reinstalling might help.\n",
      "- **Switching Containers**: If Docker is stuck on starting, try switching the backend by right-clicking the Docker icon in the system tray and selecting an appropriate backend (Windows or Linux).\n",
      "\n",
      "By following these instructions, you should be able to successfully set up and run Docker on your Windows machine. If you have any specific issues, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_20251015_161727_414680.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Now we can interact with it and do some vibe checking:\n",
    "    \n",
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# This creates a simple interactive loop where:\n",
    "# -- User enters a question\n",
    "# -- Agent processes it and responds\n",
    "# -- Complete interaction is logged to a file\n",
    "\n",
    "# Try these questions:\n",
    "# how do I use docker on windows?\n",
    "# can I join late and get a certificate?\n",
    "# what do I need to do for the certificate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899b9eb-87d4-4f59-9f5f-7d6085fcae13",
   "metadata": {},
   "source": [
    "## Adding References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b2e366d-66bf-4d2d-87dc-b901a0e2f7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When interacting with the agent, I noticed one thing: it doesn't include the reference to the original documents.\n",
    "# Let's fix it by adjusting the prompt:\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "correct link looks like this - without << /faq-main >> part :\n",
    "<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n",
    "# This is the output I now get for the question \"can I join late and get a certificate?\":\n",
    "\n",
    "# Yes, you can join the course late and still be eligible for a certificate, as long as you complete the required peer-reviewed \n",
    "# capstone projects on time. You do not need to complete the homework assignments if you join late, which allows for flexibility in participation.\n",
    "# However, please note that certificates are only awarded to those who finish the course with a ‚Äúlive‚Äù cohort; \n",
    "# they are not available for those who choose the self-paced mode. \n",
    "# This is because peer-reviewing capstone projects is a requirement that can only be done while the course is active......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a830263-fc38-4c89-856c-8ee6d4669cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I use docker on linux?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Docker on Linux, you may need to follow these general steps:\n",
      "\n",
      "1. **Installation**: Depending on your Linux distribution, you can install Docker using different methods. For example, on Ubuntu, you can use the snap command:\n",
      "\n",
      "   ```bash\n",
      "   sudo snap install docker\n",
      "   ```\n",
      "\n",
      "   This command installs Docker via snap packages, which might be available on certain versions of Ubuntu. [Learn more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/036_1b727dde32_docker-docker-not-installable-on-ubuntu.md).\n",
      "\n",
      "2. **Extra Hosts Configuration**: If you're using Docker Compose and face issues resolving `host.docker.internal`, you might need to configure it in your `docker-compose.yml` file like so:\n",
      "\n",
      "   ```yaml\n",
      "   kestra:\n",
      "     image: kestra/kestra:latest\n",
      "     extra_hosts:\n",
      "       - \"host.docker.internal:host-gateway\"\n",
      "   ```\n",
      "\n",
      "   This setup allows your container to access host services correctly. You can check more details on this configuration. [Read more here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-2/015_48b99a69ff_fix-add-extra_hosts-for-hostdockerinternal-on-linu.md).\n",
      "\n",
      "3. **Using Docker Commands**: After installing Docker, you can start using Docker commands in the terminal. Common commands include:\n",
      "   - `docker run`: To run a container.\n",
      "   - `docker ps`: To list running containers.\n",
      "   - `docker images`: To list available images.\n",
      "\n",
      "4. **Docker Compose**: If you need to orchestrate multi-container applications, Docker Compose is a useful tool. Ensure it's installed and configured properly. If after installation you have trouble using it, ensure that you might need to rename the downloaded Docker Compose binary to `docker-compose` for easy access. Here's how you can rename it:\n",
      "\n",
      "   ```bash\n",
      "   mv docker-compose-linux-x86_64 docker-compose\n",
      "   ```\n",
      "\n",
      "   This resolves issues where the command may not be recognized. [Find more about Docker Compose here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md).\n",
      "\n",
      "5. **Starting Docker**: Make sure that the Docker service is running. You can start Docker with:\n",
      "\n",
      "   ```bash\n",
      "   sudo systemctl start docker\n",
      "   ```\n",
      "\n",
      "By following these steps, you should be able to effectively use Docker on a Linux system. If you have any specific questions, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('logs/faq_agent_v2_20251015_161845_62dcb0.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())\n",
    "\n",
    "# NB - links were broken in original code - I checked generated links and model simply merge 2 http adress parts instead of replacing :\n",
    "# Wrong link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# correct link is\n",
    "# https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\n",
    "# as I manually deleted /faq-main part ...\n",
    "# I FIXED PROMPT ABOVE AND NOW IT WORKS WELL !!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeccb47-c8c9-4d27-a72e-361e4b817b0d",
   "metadata": {},
   "source": [
    "## LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b9a485-8754-46f5-9b0b-bf2051d8f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can ask your colleagues to also do a \"vibe check\", but make sure you record the data. \n",
    "# Often collecting 10-20 examples and manually inspecting them is enough to understand how your model is doing.\n",
    "# Don't be afraid of putting manual work into evaluation. Manual evaluation will help you understand edge cases, \n",
    "# learn what good responses look like and think of evaluation criteria for automated checks later.\n",
    "# For example, I manually inspected the output and noticed that references are missing. So we will later add it as one of the checks.\n",
    "# So, in our case, we can have the following checks:\n",
    "# Does the agent follow the instructions?\n",
    "# Given the question, does the answer make sense?\n",
    "# Does it include references?\n",
    "# Did the agent use the available tools?\n",
    "# We don't have to evaluate this manually. Instead, we can delegate this to AI. This technique is called \"LLM as a Judge\".\n",
    "# The idea is simple: we use one LLM to evaluate the outputs of another LLM. This works because LLMs are good at following detailed evaluation criteria.\n",
    "    \n",
    "# Our system prompt for the judge (we'll call it \"evaluation agent\" because it sounds cooler) can look like that:\n",
    "\n",
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d928c7ea-eeea-4f16-b10f-a327b6d6a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we expect a very well defined structure of the response, we can use structured output.\n",
    "# We can define a Pydantic class with the expected response structure, and the LLM will produce output that matches this schema exactly.\n",
    "    \n",
    "# This is how we do it:\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str\n",
    "\n",
    "# This code defines the structure we expect from our evaluation:\n",
    "# Each check has a name, justification, and pass/fail result\n",
    "# The overall evaluation includes a list of checks and a summary\n",
    "# Note that justification comes before check_pass. This makes \n",
    "# the LLM reason about the answer before giving the final judgment, \n",
    "# which typically leads to better evaluation quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58e8fe1-34ee-442b-9d3e-82482a50727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Pydantic AI in order to make the output follow the specified class, we use the parameter output_type:\n",
    "    \n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14079151-f6de-4756-a144-7f7b983b3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usually it's a good idea to evaluate the results of one model (in our case, \"gpt-4o-mini\") \n",
    "# with another model (e.g. \"gpt-5-nano\"). A different model can catch mistakes, reduce self-bias, \n",
    "# and give a second opinion. This makes evaluations more reliable.\n",
    "# We have the instructions, and we have the agent. In order to run the agent, it needs input. \n",
    "    \n",
    "# We'll start with a template:\n",
    "\n",
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "# We use XML markup because it's easier and more clear for LLMs to understand the input. \n",
    "# XML tags help the model see the structure and boundaries of different sections in the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812b1c26-8ac1-4c6d-b7e7-8bc2deddb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill it in. First, define a helper function for loading JSON log files:\n",
    "\n",
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data\n",
    "\n",
    "# We also add the filename in the result - it'll help us with tracking later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9fb925-6552-4464-9160-153921afde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use it:\n",
    "# log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "# here I copied full path and replaced Windows backslashes \\ to forward slashes /\n",
    "log_record = load_log_file('C:/tmp/aihero/course/logs/faq_agent_20251015_155744_df13b1.json')\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddbb8b98-cd22-4d6a-aab7-8fd86596c951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated a search tool call to fetch course-material relevant Docker-on-Linux guidance; awaiting results to craft a complete, sourced answer.\n",
      "check_name='instructions_follow' justification=\"The agent will follow the user's instruction to search course materials before answering (per <INSTRUCTIONS>). The agent hasn't yet, but will trigger a search tool in this response as required.\" check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed behavior observed. The response will rely on course materials and common Docker usage; nothing problematic. ' check_pass=True\n",
      "check_name='answer_relevant' justification='The answer will address Docker use on Linux. ' check_pass=True\n",
      "check_name='answer_clear' justification='The final answer will present steps clearly. ' check_pass=True\n",
      "check_name='answer_citations' justification='The answer will cite course material results from search; or at least reference them. ' check_pass=True\n",
      "check_name='completeness' justification='The answer will cover installation, starting service, permissions, running container, docker-compose, etc. ' check_pass=True\n",
      "check_name='tool_call_search' justification='We will call a search tool as required. ' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "# The user input is ready and we can test it!\n",
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)\n",
    "\n",
    "# This code:\n",
    "# Loads a saved interaction log\n",
    "# Extracts the key components (instructions, question, answer, full log)\n",
    "# Formats them into the evaluation prompt\n",
    "# Runs the evaluation agent\n",
    "# Prints the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "817018ac-7ec2-48ff-af17-8be761f53f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're putting the entire conversation log into the prompt, which is not really necessary. \n",
    "# We can reduce it to make it less verbose.\n",
    "# For example, like that:\n",
    "\n",
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified\n",
    "\n",
    "# We make it simpler:\n",
    "# remove timestamps and IDs that aren't needed for evaluation\n",
    "# replace actual search results with a placeholder\n",
    "# keep only the essential structure\n",
    "# This is helpful because it reduces the number of tokens we send to the evaluation model, \n",
    "# which lowers the costs and speeds up evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5eab01e7-2cbd-45ff-ba09-2d7a5d55d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put everything together:\n",
    "\n",
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "# log_record = load_log_file('./logs/faq_agent_v2_20250926_072928_467470.json')\n",
    "# here I copied full path and replaced Windows backslashes \\ to forward slashes /\n",
    "log_record = load_log_file('C:/tmp/aihero/course/logs/faq_agent_20251015_155744_df13b1.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)\n",
    "\n",
    "# We know how to log our data and how to run evals on our logs.\n",
    "# Great. But how do we get more data to get a better understanding of the performance of our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a16893a-d098-465c-bc2e-2da061c9bef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', justification='The user instruction required using a search tool to fetch course materials before answering; a text_search tool call is present in the log, indicating this was done.', check_pass=True), EvaluationCheck(check_name='instructions_avoid', justification='No disallowed actions or content were involved; the answer stayed within typical Docker usage guidance.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"The answer directly explains how to install, start, configure, and use Docker on Linux, addressing the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='The steps are organized and include concrete commands, making the guidance easy to follow.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='No citations were included, but citations are not strictly required for this general setup guide.', check_pass=True), EvaluationCheck(check_name='completeness', justification='Covers installation, service management, user permissions, basic commands, running containers, Compose, and host.docker.internal note.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='The log shows a text_search tool invocation for this topic.', check_pass=True)], summary='All checklist criteria are satisfied. The agent followed the search instruction, addressed the Docker-on-Linux setup comprehensively, and avoided issues. Citations were not required for this general guidance.')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e9eb4-8172-4d8b-ab99-6dbab87d4d34",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "662035b2-f456-47de-a986-33786eb44b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation\n",
    "# We can ask AI to help. What if we used it for generating more questions? Let's do that.\n",
    "# We can sample some records from our database. Then for each record, ask an LLM to generate a question based on the record. \n",
    "# We use this question as input to our agent and log the answers.\n",
    "\n",
    "# Let‚Äôs start by defining the question generator:\n",
    "\n",
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n",
    "\n",
    "# This prompt is designed for our specific use case (data engineering course FAQ). You should adjust it for your project.\n",
    "# We will send it a bunch of records, and it will generate a question from each of them.\n",
    "# Note: we use a simple way of generating questions. We can use a more complex approach where \n",
    "# we also track the source (filename) of the question. If we do it, we can later check \n",
    "# if this file was retrieved and cited in the answer. But we won't do it today to make things simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73dd1ca3-723c-48dc-a5b9-3c4a76243f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's sample 10 records from our dataset using Python's built-in random.sample function:\n",
    "\n",
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec01f3c1-102a-4e70-b3be-c805675f83ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do I install the `dlt[duckdb]` package to run the provided code?',\n",
       " 'What does horizontal scaling involve in the context of data consumption?',\n",
       " 'Where can I find the `rides.csv` data used by the `producer.py` Python program?',\n",
       " 'Can you explain how to unzip a parquet file and use it with pandas?',\n",
       " 'Does BigQuery support real-time data streaming capabilities?',\n",
       " 'How can I switch to in-file storage for my dataset?',\n",
       " 'What should I do if I face date type errors after uploading FHV 2019 CSV files to BigQuery?',\n",
       " 'What can cause a `FileNotFoundException` when writing parquet files in PySpark, and how can I solve this?',\n",
       " 'How does PySpark handle the difference between two timestamps, and how can I express that duration in hours?',\n",
       " 'What is the purpose of the Key Vault in Azure Cloud, and how can it be used for storing sensitive information?']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34dc9f93-ee30-4819-bdac-3a513e38b759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc792a620bc40918f2ffbcb36488c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I install the `dlt[duckdb]` package to run the provided code?\n",
      "To install the `dlt[duckdb]` package, you can run the following command in your terminal or Jupyter notebook:\n",
      "\n",
      "```bash\n",
      "pip install dlt[duckdb]\n",
      "```\n",
      "\n",
      "If you are using it locally, ensure that `duckdb` is installed before loading the `duckdb` package as follows:\n",
      "\n",
      "```bash\n",
      "pip install \"dlt[duckdb]\"\n",
      "```\n",
      "\n",
      "This will ensure that all the necessary dependencies for running your code are installed properly. \n",
      "\n",
      "For more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/003_122d2b0aed_how-do-i-install-the-necessary-dependencies-to-run.md).\n",
      "\n",
      "What does horizontal scaling involve in the context of data consumption?\n",
      "In the context of data consumption, horizontal scaling involves distributing the process of consuming messages across multiple consumers. This approach allows for the parallel consumption of data, enhancing processing speed and increasing overall system throughput. Instead of relying on a single consumer to handle all the data, you can deploy additional consumer instances, thereby effectively managing higher volumes of data and ensuring that the system can scale out as needed.\n",
      "\n",
      "This concept is particularly relevant in systems like Apache Kafka, which supports horizontal scaling options, enabling better management of data streams and consumer loads.\n",
      "\n",
      "For more information, you can refer to the source material: [Kafka homework Q3](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/013_443b9afd04_kafka-homework-q3-there-are-options-that-support-t.md).\n",
      "\n",
      "Where can I find the `rides.csv` data used by the `producer.py` Python program?\n",
      "You can find the `rides.csv` data used by the `producer.py` Python program at the following link: [Rides CSV File](https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv) [source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/011_759b5d80ec_kafka-python-videos-ridescsv.md). \n",
      "\n",
      "If this file is missing, it is suggested to copy it from the Java example directory at this path: `data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv` [source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/009_dcdd7eda4d_resourcesridescsv-is-missing.md).\n",
      "\n",
      "Can you explain how to unzip a parquet file and use it with pandas?\n",
      "To unzip a Parquet file and use it with pandas, you generally do not need to unzip a Parquet file because Parquet is a columnar storage file format that pandas can read directly. However, if you are working with a compressed file (like `.gz`), you would handle it slightly differently. Here‚Äôs how you can work with Parquet files using pandas:\n",
      "\n",
      "1. If you have a `.parquet` file directly, you can read it using:\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   \n",
      "   df = pd.read_parquet('your_file.parquet')\n",
      "   ```\n",
      "\n",
      "2. If you have a compressed `.csv.gz` file that you want to convert to a DataFrame:\n",
      "   - First, you would unzip it:\n",
      "     ```bash\n",
      "     gunzip your_file.csv.gz\n",
      "     ```\n",
      "   - Then, you can read the resulting CSV file:\n",
      "     ```python\n",
      "     df = pd.read_csv('your_file.csv')\n",
      "     ```\n",
      "\n",
      "For example, to download a Parquet file and work with it directly, you can do this:\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Step to define the parquet file name\n",
      "parquet_name = 'output.parquet'\n",
      "\n",
      "# Command to download the parquet file\n",
      "os.system(f\"wget your_url -O {parquet_name}\")\n",
      "\n",
      "# Reading the Parquet file into a DataFrame\n",
      "df = pd.read_parquet(parquet_name)\n",
      "```\n",
      "\n",
      "This approach allows you to easily manage and analyze your data using pandas without needing to convert it into another format. Make sure you have the necessary libraries (`pyarrow` or `fastparquet`) installed for reading Parquet files in pandas.\n",
      "\n",
      "For more specific examples, refer to the information found [here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/004_8585d2d7f4_taxi-data-unzip-parquet-file.md).\n",
      "\n",
      "Does BigQuery support real-time data streaming capabilities?\n",
      "Yes, BigQuery supports real-time data streaming capabilities. This feature allows you to ingest streaming data into BigQuery, enabling real-time analytics, which can be beneficial for applications that require timely insights. While specific real-time analytics use cases may not be explicitly discussed, the capacity for real-time data streaming is recognized within BigQuery's functionality.\n",
      "\n",
      "For more details, refer to the source: [BigQuery real-time analytics](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-3/013_57303f9d80_gcp-bq-can-i-use-bigquery-for-real-time-analytics.md).\n",
      "\n",
      "How can I switch to in-file storage for my dataset?\n",
      "To switch to in-file storage for your dataset when using DuckDB, you can follow these instructions mentioned in the course materials:\n",
      "\n",
      "1. **Specify the file storage method**: Instead of using the default in-memory storage, you can opt for in-file storage when you initialize your DuckDB connection.\n",
      "\n",
      "Here's a snippet from the materials:\n",
      "> Alternatively, you can switch to in-file storage with:\n",
      "\n",
      "Unfortunately, the exact command or syntax to implement this was not extracted, but you typically specify a path to a file when creating your DuckDB connection. This means using a format similar to this:\n",
      "\n",
      "```python\n",
      "import duckdb\n",
      "con = duckdb.connect('my_database.duckdb')\n",
      "```\n",
      "\n",
      "This command will create or connect to an in-file database called `my_database.duckdb`.\n",
      "\n",
      "For the full context and additional examples, you might refer to the original document: [DuckDB In-Memory database with dlt](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/005_60b8576833_how-can-i-use-duckdb-in-memory-database-with-dlt.md). \n",
      "\n",
      "Feel free to ask more questions if you need additional details!\n",
      "\n",
      "What should I do if I face date type errors after uploading FHV 2019 CSV files to BigQuery?\n",
      "If you encounter date type errors after uploading the FHV 2019 CSV files to BigQuery, here‚Äôs what you can do:\n",
      "\n",
      "1. **Define Dates as Strings**: When creating an external table in BigQuery, define the `pickup_datetime` and `dropoff_datetime` fields as `STRING`. This will avoid any parsing errors related to date types during the initial upload:\n",
      "\n",
      "   ```sql\n",
      "   CREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata` (\n",
      "       dispatching_base_num STRING,\n",
      "       pickup_datetime STRING,\n",
      "       dropoff_datetime STRING,\n",
      "       PUlocationID STRING,\n",
      "       DOlocationID STRING,\n",
      "       SR_Flag STRING,\n",
      "       Affiliated_base_number STRING\n",
      "   )\n",
      "   OPTIONS (\n",
      "       format = 'csv',\n",
      "       uris = ['gs://bucket/*.csv']\n",
      "   );\n",
      "   ```\n",
      "\n",
      "2. **Convert to Timestamp**: Later, when creating models (for example, in dbt), you can convert these string fields to `TIMESTAMP` using the following SQL syntax:\n",
      "\n",
      "   ```sql\n",
      "   SELECT \n",
      "       TIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\n",
      "       TIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime\n",
      "   FROM \n",
      "       {{ ref('stg_fhv_tripdata') }}\n",
      "   ```\n",
      "\n",
      "This method allows you to handle the date type errors more gracefully during the upload process and provides a way to ensure the data is in the correct format for further processing [source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-4/075_8c9b0fdaa5_invalid-date-types-after-ingesting-fhv-data-throug.md).\n",
      "\n",
      "What can cause a `FileNotFoundException` when writing parquet files in PySpark, and how can I solve this?\n",
      "A `FileNotFoundException` when writing Parquet files in PySpark can occur due to several reasons. One common cause is related to the way Spark handles file paths and operations on them, especially in the context of lazy evaluation.\n",
      "\n",
      "**Issue:**\n",
      "When you attempt to write to a Parquet file with mode set to \"overwrite,\" Spark may delete the existing files it is trying to read because of the lazy transformations it employs. Thus, you may encounter an error if Spark tries to read a file that has just been deleted.\n",
      "\n",
      "**Example of the Error:**\n",
      "```python\n",
      "# Code executed:\n",
      "df = spark.read.parquet(pq_path)\n",
      "\n",
      "# ‚Ä¶ some operations on df ‚Ä¶\n",
      "\n",
      "df.write.parquet(pq_path, mode=\"overwrite\")\n",
      "\n",
      "# Error:\n",
      "# java.io.FileNotFoundException: File file:/path/to/your.data.parquet does not exist\n",
      "```\n",
      "\n",
      "**Solution:**\n",
      "1. **Write to a different directory:** Instead of writing to the same path where you read the Parquet files, you can specify a temporary or different directory for your output.\n",
      "   ```python\n",
      "   df.write.parquet(pq_path_temp, mode=\"overwrite\")\n",
      "   ```\n",
      "\n",
      "By writing to a different path, you avoid the conflict of trying to read and overwrite the same file simultaneously, which eliminates the `FileNotFoundException` error.\n",
      "\n",
      "For more detailed information on this error and its resolution, you can refer to the source document here: [FileNotFoundException](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-5/017_ef01c39d63_error-javaiofilenotfoundexception.md).\n",
      "\n",
      "How does PySpark handle the difference between two timestamps, and how can I express that duration in hours?\n",
      "In PySpark, the difference between two `TimestampType` values is converted into Python's native `datetime.timedelta` object. This object represents the duration in terms of days, seconds, and microseconds. To express this duration in hours, you need to manually convert it, as follows:\n",
      "\n",
      "1. **Using `timedelta`:** After obtaining the timedelta object, you can access its total seconds using the `total_seconds()` method and then convert that to hours by dividing by 3600 (the number of seconds in an hour).\n",
      "\n",
      "   ```python\n",
      "   duration_in_hours = (timestamp2 - timestamp1).total_seconds() / 3600\n",
      "   ```\n",
      "\n",
      "2. **Using the `datediff` SQL function:** This function returns the difference in days between two dates. You can multiply the result by 24 to get the duration in hours:\n",
      "\n",
      "   ```sql\n",
      "   SELECT DATEDIFF(dropoff_datetime, pickup_datetime) * 24 AS duration_in_hours\n",
      "   ```\n",
      "\n",
      "These methods allow you to effectively calculate the difference between two timestamps and express that duration in hours. \n",
      "\n",
      "For detailed information, refer to the source: [Homework: how to convert the time difference of two timestamps to hours](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-5/053_1da0437718_homework-how-to-convert-the-time-difference-of-two.md).\n",
      "\n",
      "What is the purpose of the Key Vault in Azure Cloud, and how can it be used for storing sensitive information?\n",
      "The Azure Key Vault is designed to securely store and manage sensitive information such as credentials, passwords, and other secrets. Its primary purpose is to prevent the exposure of these sensitive data elements, ensuring they can be safely used across various Azure applications and services without being hardcoded or exposed in code or configuration files.\n",
      "\n",
      "For instance, if you have a SQL database password that you want to keep confidential, you can store this password in the Azure Key Vault under a specified name. Then, this stored secret can be retrieved and used by other Azure services securely, allowing for a more secure application architecture.\n",
      "\n",
      "For more detailed information, you can refer to this resource: [Key Vault in Azure cloud stack](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/project/012_ec5e405fa6_key-vault-in-azure-cloud-stack.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now we simply iterate over each of the question, ask our agent and log the results:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()\n",
    "\n",
    "# We can repeat it multiple times until we have enough data. \n",
    "# Around 100 should be good for a start, but today we can just \n",
    "# continue with the 10 log records we already generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c4dc22e-f618-46e9-a1c1-b9e7e4405429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using AI for generating test data is quite powerful. It can help us get data faster \n",
    "# and sometimes cover edge cases we won't think about.\n",
    "# There are limitations too:\n",
    "# AI-generated questions might not reflect real user behavior\n",
    "# It may miss important edge cases that only real users encounter\n",
    "# They may not capture the full complexity of real user queries\n",
    "# The logs are ready, so we can run evaluation on them with our evaluation agent.\n",
    "    \n",
    "# First, collect all the AI-generated logs for the v2 agent:\n",
    "\n",
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8b8e917-b6d2-4f5b-ac54-3740e0a80fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e98ddb7-c9e8-431a-8dc8-faea0aed6300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0898e827574b4f84b3213bf8f691a38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And evaluate them:\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))\n",
    "\n",
    "# This code:\n",
    "# Loops through each AI-generated log\n",
    "# Runs our evaluation agent on it\n",
    "# Stores both the original log and evaluation result\n",
    "# There are ways to speed this up, but we won't cover them in detail here. For example, you can try this:\n",
    "# Don't ask for justification - this makes evaluation faster but slightly lower quality\n",
    "# Parallelize execution - you can ask ChatGPT how to do this with async/await\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca6fb22d-5625-41af-84aa-4aab6e188971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent_name': 'faq_agent_v2',\n",
       "  'system_prompt': ['You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\ncorrect link looks like this - without << /faq-main >> part :\\n<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.'],\n",
       "  'provider': 'openai',\n",
       "  'model': 'gpt-4o-mini',\n",
       "  'tools': ['text_search'],\n",
       "  'messages': [{'parts': [{'content': 'How do I install the `dlt[duckdb]` package to run the provided code?',\n",
       "      'timestamp': '2025-10-15T20:00:35.714451+00:00',\n",
       "      'part_kind': 'user-prompt'}],\n",
       "    'instructions': 'You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\ncorrect link looks like this - without << /faq-main >> part :\\n<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.',\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'args': '{\"query\":\"install dlt[duckdb] package\"}',\n",
       "      'tool_call_id': 'call_Uj1ZvK4EY0tvfxkshyiOGEJc',\n",
       "      'part_kind': 'tool-call'}],\n",
       "    'usage': {'input_tokens': 292,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 21,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {'accepted_prediction_tokens': 0,\n",
       "      'audio_tokens': 0,\n",
       "      'reasoning_tokens': 0,\n",
       "      'rejected_prediction_tokens': 0}},\n",
       "    'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "    'timestamp': '2025-10-15T20:00:35+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'openai',\n",
       "    'provider_details': {'finish_reason': 'tool_calls'},\n",
       "    'provider_response_id': 'chatcmpl-CR1xj186AOA8cmv0vxfbl0DoAbLpf',\n",
       "    'finish_reason': 'tool_call'},\n",
       "   {'parts': [{'tool_name': 'text_search',\n",
       "      'content': [{'id': '122d2b0aed',\n",
       "        'question': 'How do I install the necessary dependencies to run the code?',\n",
       "        'sort_order': 3,\n",
       "        'content': 'To run the provided code, ensure that the `dlt[duckdb]` package is installed. You can do this by executing the following installation command in a Jupyter notebook:\\n\\n```bash\\n!pip install dlt[duckdb]\\n```\\n\\nIf you‚Äôre installing it locally, make sure to also have `duckdb` installed before the `duckdb` package is loaded:\\n\\n```zsh\\npip install \"dlt[duckdb]\"\\n```',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/003_122d2b0aed_how-do-i-install-the-necessary-dependencies-to-run.md'},\n",
       "       {'id': 'eaae17dfea',\n",
       "        'question': 'Embedding dlt into Kestra',\n",
       "        'sort_order': 12,\n",
       "        'content': '```yaml\\nid: dlt_ingestion\\n\\nnamespace: my.dlt\\n\\ndescription: \"Run dlt pipeline with Kestra\"\\n\\ntasks:\\n\\n- id: run_dlt\\n\\n  type: io.kestra.plugin.scripts.python.Commands\\n\\n  commands:\\n\\n  - |\\n\\n    import dlt\\n\\n    from my_dlt_pipeline import load_data  # Import your dlt function\\n\\n    pipeline = dlt.pipeline(\\n\\n      pipeline_name=\"kestra_pipeline\",\\n\\n      destination=\"duckdb\",\\n\\n      dataset_name=\"kestra_dataset\"\\n\\n    )\\n\\n    info = pipeline.run(load_data())\\n\\n    print(info)\\n```',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/012_eaae17dfea_embedding-dlt-into-kestra.md'},\n",
       "       {'id': '27fa940238',\n",
       "        'question': 'Problem with importing the dlt or dlt.sources module',\n",
       "        'sort_order': 7,\n",
       "        'content': 'Make sure you don‚Äôt have a `dlt.py` file saved in the same directory as your working file.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/007_27fa940238_problem-with-importing-the-dlt-or-dltsources-modul.md'},\n",
       "       {'id': '60b8576833',\n",
       "        'question': 'How can I use DuckDB In-Memory database with dlt?',\n",
       "        'sort_order': 5,\n",
       "        'content': 'Alternatively, you can switch to in-file storage with:',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/005_60b8576833_how-can-i-use-duckdb-in-memory-database-with-dlt.md'},\n",
       "       {'id': 'f48b484cd4',\n",
       "        'question': 'Embedding dlt into Apache Airflow',\n",
       "        'sort_order': 11,\n",
       "        'content': 'To integrate a `dlt` pipeline into Apache Airflow, follow this example:\\n\\n```python\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom datetime import datetime, timedelta\\nimport dlt\\nfrom my_dlt_pipeline import load_data  # Import your dlt pipeline function\\n\\ndefault_args = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"start_date\": datetime(2024, 2, 16),\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5),\\n}\\n\\ndef run_dlt_pipeline():\\n    pipeline = dlt.pipeline(\\n        pipeline_name=\"my_pipeline\",\\n        destination=\"duckdb\",  # Change this based on your database\\n        dataset_name=\"my_dataset\"\\n    )\\n    info = pipeline.run(load_data())\\n    print(info)  # Logs for debugging\\n\\nwith DAG(\\n    \"dlt_airflow_pipeline\",\\n    default_args=default_args,\\n    schedule_interval=\"@daily\",\\n    catchup=False,\\n) as dag:\\n    run_dlt_task = PythonOperator(\\n        task_id=\"run_dlt_pipeline\",\\n        python_callable=run_dlt_pipeline,\\n    )\\n    run_dlt_task\\n```\\n\\nEnsure to replace `\"duckdb\"` with your actual database name and adjust the `load_data` function according to your specific `dlt` pipeline.',\n",
       "        'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/011_f48b484cd4_embedding-dlt-into-apache-airflow.md'}],\n",
       "      'tool_call_id': 'call_Uj1ZvK4EY0tvfxkshyiOGEJc',\n",
       "      'metadata': None,\n",
       "      'timestamp': '2025-10-15T20:00:37.246855+00:00',\n",
       "      'part_kind': 'tool-return'}],\n",
       "    'instructions': 'You are a helpful assistant for a course.  \\n\\nUse the search tool to find relevant information from the course materials before answering questions.  \\n\\nIf you can find specific information through search, use it to provide accurate answers.\\n\\nAlways include references by citing the filename of the source material you used.  \\nWhen citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\\ncorrect link looks like this - without << /faq-main >> part :\\n<< https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md >> \\nFormat: [LINK TITLE](FULL_GITHUB_LINK)\\n\\nIf the search doesn\\'t return relevant results, let the user know and provide general guidance.',\n",
       "    'kind': 'request'},\n",
       "   {'parts': [{'content': 'To install the `dlt[duckdb]` package, you can run the following command in your terminal or Jupyter notebook:\\n\\n```bash\\npip install dlt[duckdb]\\n```\\n\\nIf you are using it locally, ensure that `duckdb` is installed before loading the `duckdb` package as follows:\\n\\n```bash\\npip install \"dlt[duckdb]\"\\n```\\n\\nThis will ensure that all the necessary dependencies for running your code are installed properly. \\n\\nFor more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/003_122d2b0aed_how-do-i-install-the-necessary-dependencies-to-run.md).',\n",
       "      'id': None,\n",
       "      'part_kind': 'text'}],\n",
       "    'usage': {'input_tokens': 1279,\n",
       "     'cache_write_tokens': 0,\n",
       "     'cache_read_tokens': 0,\n",
       "     'output_tokens': 163,\n",
       "     'input_audio_tokens': 0,\n",
       "     'cache_audio_read_tokens': 0,\n",
       "     'output_audio_tokens': 0,\n",
       "     'details': {'accepted_prediction_tokens': 0,\n",
       "      'audio_tokens': 0,\n",
       "      'reasoning_tokens': 0,\n",
       "      'rejected_prediction_tokens': 0}},\n",
       "    'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "    'timestamp': '2025-10-15T20:00:36+00:00',\n",
       "    'kind': 'response',\n",
       "    'provider_name': 'openai',\n",
       "    'provider_details': {'finish_reason': 'stop'},\n",
       "    'provider_response_id': 'chatcmpl-CR1xk54k6Au3h8KFrkDOTKI8mJj1i',\n",
       "    'finish_reason': 'stop'}],\n",
       "  'source': 'ai-generated',\n",
       "  'log_file': WindowsPath('logs/faq_agent_v2_20251015_200036_d284b1.json')},\n",
       " EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', justification=\"The answer did not format citations according to the required [LINK TITLE](FULL_GITHUB_LINK) style and did not replace the 'faq-main' with the full path, so it did not fully follow the user's instructions.\", check_pass=False), EvaluationCheck(check_name='instructions_avoid', justification='No disallowed content or actions; content is safe.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification=\"Installation commands addressed the user's question.\", check_pass=True), EvaluationCheck(check_name='answer_clear', justification='Commands are clear and actionable.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='Citations were not formatted as per the required markdown citation format; a link was provided but not formatted as [TITLE](URL).', check_pass=False), EvaluationCheck(check_name='completeness', justification='Covers the installation command and a note about dependencies; generally complete for most users.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='No search tool invoked in this turn; the response relied on a known command and a link, but the system instruction requires a search tool to be used when possible.', check_pass=False)], summary='The assistant provides the correct installation commands but fails to format citations per the specified format and does not perform a search in this turn, which was requested by the course instructions.'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The results are collected, but we need to display them and also calculate some statistics. \n",
    "# The best tool for doing this is Pandas. We already should have it because minsearch depends on it. \n",
    "# But we can make it an explicit dependency:\n",
    "\n",
    "# uv add pandas\n",
    "\n",
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0dc3e947-09ae-44da-a07d-a8a4c7a1b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Our data is not ready to be converted to a Pandas DataFrame. \n",
    "# We first need to transform it a little. Let‚Äôs do it:\n",
    "\n",
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# This code:\n",
    "# Extracts key information from each log (file, question, answer)\n",
    "# Converts the evaluation checks into a dictionary format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41abcb8f-be2e-47ff-839f-f2baa7186344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'faq_agent_v2_20251015_200046_c56d1b.json',\n",
       " 'question': 'Where can I find the `rides.csv` data used by the `producer.py` Python program?',\n",
       " 'answer': 'You can find the `rides.csv` data used by the `producer.py` Python program at the following link: [Rides CSV File](https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv) [source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/011_759b5d80ec_kafka-python-videos-ridescsv.md). \\n\\nIf this file is missing, it is suggested to copy it from the Java example directory at this path: `data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv` [source](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/009_dcdd7eda4d_resourcesridescsv-is-missing.md).',\n",
       " 'instructions_follow': True,\n",
       " 'instructions_avoid': True,\n",
       " 'answer_relevant': True,\n",
       " 'answer_clear': True,\n",
       " 'answer_citations': True,\n",
       " 'completeness': True,\n",
       " 'tool_call_search': False}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccb99072-ade7-48b9-8d5a-ab9f5bf081f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>instructions_follow</th>\n",
       "      <th>instructions_avoid</th>\n",
       "      <th>answer_relevant</th>\n",
       "      <th>answer_clear</th>\n",
       "      <th>answer_citations</th>\n",
       "      <th>completeness</th>\n",
       "      <th>tool_call_search</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faq_agent_v2_20251015_200036_d284b1.json</td>\n",
       "      <td>How do I install the `dlt[duckdb]` package to ...</td>\n",
       "      <td>To install the `dlt[duckdb]` package, you can ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faq_agent_v2_20251015_200041_996004.json</td>\n",
       "      <td>What does horizontal scaling involve in the co...</td>\n",
       "      <td>In the context of data consumption, horizontal...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faq_agent_v2_20251015_200046_c56d1b.json</td>\n",
       "      <td>Where can I find the `rides.csv` data used by ...</td>\n",
       "      <td>You can find the `rides.csv` data used by the ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>faq_agent_v2_20251015_200053_37c351.json</td>\n",
       "      <td>Can you explain how to unzip a parquet file an...</td>\n",
       "      <td>To unzip a Parquet file and use it with pandas...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faq_agent_v2_20251015_200103_84f227.json</td>\n",
       "      <td>Does BigQuery support real-time data streaming...</td>\n",
       "      <td>Yes, BigQuery supports real-time data streamin...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>faq_agent_v2_20251015_200107_8a1eb5.json</td>\n",
       "      <td>How can I switch to in-file storage for my dat...</td>\n",
       "      <td>To switch to in-file storage for your dataset ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faq_agent_v2_20251015_200114_514722.json</td>\n",
       "      <td>What should I do if I face date type errors af...</td>\n",
       "      <td>If you encounter date type errors after upload...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faq_agent_v2_20251015_200123_9706cd.json</td>\n",
       "      <td>What can cause a `FileNotFoundException` when ...</td>\n",
       "      <td>A `FileNotFoundException` when writing Parquet...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faq_agent_v2_20251015_200133_50a0fd.json</td>\n",
       "      <td>How does PySpark handle the difference between...</td>\n",
       "      <td>In PySpark, the difference between two `Timest...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>faq_agent_v2_20251015_200141_922cb1.json</td>\n",
       "      <td>What is the purpose of the Key Vault in Azure ...</td>\n",
       "      <td>The Azure Key Vault is designed to securely st...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file  \\\n",
       "0  faq_agent_v2_20251015_200036_d284b1.json   \n",
       "1  faq_agent_v2_20251015_200041_996004.json   \n",
       "2  faq_agent_v2_20251015_200046_c56d1b.json   \n",
       "3  faq_agent_v2_20251015_200053_37c351.json   \n",
       "4  faq_agent_v2_20251015_200103_84f227.json   \n",
       "5  faq_agent_v2_20251015_200107_8a1eb5.json   \n",
       "6  faq_agent_v2_20251015_200114_514722.json   \n",
       "7  faq_agent_v2_20251015_200123_9706cd.json   \n",
       "8  faq_agent_v2_20251015_200133_50a0fd.json   \n",
       "9  faq_agent_v2_20251015_200141_922cb1.json   \n",
       "\n",
       "                                            question  \\\n",
       "0  How do I install the `dlt[duckdb]` package to ...   \n",
       "1  What does horizontal scaling involve in the co...   \n",
       "2  Where can I find the `rides.csv` data used by ...   \n",
       "3  Can you explain how to unzip a parquet file an...   \n",
       "4  Does BigQuery support real-time data streaming...   \n",
       "5  How can I switch to in-file storage for my dat...   \n",
       "6  What should I do if I face date type errors af...   \n",
       "7  What can cause a `FileNotFoundException` when ...   \n",
       "8  How does PySpark handle the difference between...   \n",
       "9  What is the purpose of the Key Vault in Azure ...   \n",
       "\n",
       "                                              answer instructions_follow  \\\n",
       "0  To install the `dlt[duckdb]` package, you can ...               False   \n",
       "1  In the context of data consumption, horizontal...                True   \n",
       "2  You can find the `rides.csv` data used by the ...                True   \n",
       "3  To unzip a Parquet file and use it with pandas...               False   \n",
       "4  Yes, BigQuery supports real-time data streamin...                True   \n",
       "5  To switch to in-file storage for your dataset ...                True   \n",
       "6  If you encounter date type errors after upload...                True   \n",
       "7  A `FileNotFoundException` when writing Parquet...               False   \n",
       "8  In PySpark, the difference between two `Timest...                True   \n",
       "9  The Azure Key Vault is designed to securely st...                 NaN   \n",
       "\n",
       "  instructions_avoid answer_relevant answer_clear answer_citations  \\\n",
       "0               True            True         True            False   \n",
       "1               True            True         True             True   \n",
       "2               True            True         True             True   \n",
       "3               True            True         True            False   \n",
       "4               True            True         True             True   \n",
       "5               True            True         True             True   \n",
       "6               True            True         True             True   \n",
       "7               True            True         True             True   \n",
       "8               True            True         True             True   \n",
       "9                NaN             NaN          NaN              NaN   \n",
       "\n",
       "  completeness tool_call_search  \n",
       "0         True            False  \n",
       "1         True             True  \n",
       "2         True            False  \n",
       "3         True            False  \n",
       "4         True            False  \n",
       "5         True            False  \n",
       "6         True             True  \n",
       "7        False            False  \n",
       "8        False             True  \n",
       "9          NaN              NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now each row is a simple key-value dictionary, so we can create a DataFrame:\n",
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "df_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37684c93-8a39-4ce2-9fba-53e4b2ea64c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denis\\AppData\\Local\\Temp\\ipykernel_1660\\2760972573.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_evals2 = df_evals.replace({True: 1, False: 0})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "instructions_follow    0.67\n",
       "instructions_avoid     1.00\n",
       "answer_relevant        1.00\n",
       "answer_clear           1.00\n",
       "answer_citations       0.78\n",
       "completeness           0.78\n",
       "tool_call_search       0.33\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at individual records and see which checks are False.\n",
    "# But it's also useful to look at the overall stats:\n",
    "\n",
    "# df_evals.mean(numeric_only=True)\n",
    "# talking to ChatGPT to modify it:\n",
    "df_evals2 = df_evals.replace({True: 1, False: 0})\n",
    "round(df_evals2.mean(numeric_only=True), 2)\n",
    "# df_evals2\n",
    "# This calculates the average pass rate for each check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1042adf0-b75e-4be5-a6cd-1ba8932941f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code above calculates the average pass rate for each check like so:\n",
    "# instructions_follow    0.3\n",
    "# instructions_avoid     1.0\n",
    "# answer_relevant        1.0\n",
    "# answer_clear           1.0\n",
    "# answer_citations       0.3\n",
    "# completeness           0.7\n",
    "# tool_call_search       1.0\n",
    "\n",
    "# This tells us:\n",
    "# Only 30% of responses follow instructions completely\n",
    "# All responses avoid forbidden actions (good!)\n",
    "# All responses are relevant and clear (great!)\n",
    "# Only 30% include proper citations (needs improvement)\n",
    "# 70% of responses are complete\n",
    "# All responses use the search tool (as expected)\n",
    "# For us, the most important check is answer_relevant. \n",
    "# This tells us whether the agent actually answers the user's question. \n",
    "# If this score was low, it‚Äôd mean that our agent is not ready.\n",
    "# We now know how to evaluate our agent. What can we do with it now?\n",
    "# Many things:\n",
    "# Decide if this quality is good enough for deployment\n",
    "# Evaluate different chunking approaches and search\n",
    "# See if changing a prompt leads to any improvements.\n",
    "# The algorithm is simple:\n",
    "# Collect data for evaluation and keep this dataset fixed\n",
    "# Run different versions of your agent for this dataset\n",
    "# Compare key metrics to decide which version is better\n",
    "# Evaluation is a very powerful tool and we should use it when possible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bd01d-576e-4b07-a85f-51b28961f062",
   "metadata": {},
   "source": [
    "## Evaluating functions and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "376e510c-0156-4b90-8705-8dc02a064e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating functions and tools\n",
    "# Also, we can (and should) evaluate our tools separately from evaluating the agent.\n",
    "# If it's code, we need to cover it with unit and integration tests.\n",
    "# We also have the search function, which we can evaluate using standard information retrieval metrics. For example:\n",
    "# Precision and Recall: How many relevant results were retrieved vs. how many relevant results were missed\n",
    "# Hit Rate: Percentage of queries that return at least one relevant result\n",
    "# MRR (Mean Reciprocal Rank): Reflects the position of the first relevant result in the ranking\n",
    "# This is how we can implement hitrate and MRR calculation in Python:\n",
    "# def evaluate_search_quality(search_function, test_queries):\n",
    "#     results = []\n",
    "    \n",
    "#     for query, expected_docs in test_queries:\n",
    "#         search_results = search_function(query, num_results=5)\n",
    "        \n",
    "#         # Calculate hit rate\n",
    "#         relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "#         # Calculate MRR\n",
    "#         for i, doc in enumerate(search_results):\n",
    "#             if doc['filename'] in expected_docs:\n",
    "#                 mrr = 1 / (i + 1)\n",
    "#                 break\n",
    "#         else:\n",
    "#             mrr = 0\n",
    "            \n",
    "#         results.append({\n",
    "#             'query': query,\n",
    "#             'hit': relevant_found,\n",
    "#             'mrr': mrr\n",
    "#         })\n",
    "#     return results\n",
    "\n",
    "# We won't do it today, but these ideas and the code will be useful when you implement a real agent project with search.\n",
    "# It's useful because it'll helps us make guided decisions about:\n",
    "# When to use text vs. vector vs. hybrid search\n",
    "# What are the best parameters for our search\n",
    "# You can ask ChatGPT to learn more about information retrieval evaluation metrics.\n",
    "\n",
    "# This was a very long lesson, but an important one. We finished it, and evaluated our agent. It‚Äôs good for deployment, so tomorrow we‚Äôll create an UI for it and deploy it to the internet.\n",
    "# Homework\n",
    "# Create an evaluation system for your agent\n",
    "# Collect at least 10 interaction logs\n",
    "# Set up automated evaluation using LLM as a judge\n",
    "# Test different system prompts and compare results\n",
    "# Make a post on social media about your evaluation process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48345e12-6b42-4c37-a28e-bb9db2bc257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf81553-1ad1-4f23-bec5-2eb6285bd270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
