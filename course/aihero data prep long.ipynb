{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1cac94-1f14-44f9-a8bc-1c359b395889",
   "metadata": {},
   "source": [
    "## Download data and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17d7bfa-3616-4e22-b579-35b48511c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd76b1-7566-4643-b71c-495a3e5f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download the repository as a zip file. GitHub provides a convenient URL format for this:\n",
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "# NB!! This code downloads the file to memory, not to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb396e-1759-4995-8203-bc5e4fefbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e3e28-5a9c-4a25-b2de-eda7c108e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fe6486-4ce8-4346-9963-2b0e4f12faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Implementation\n",
    "# Let's now put everything together into a reusable function:\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ba8192-69c9-4667-9a4c-ae558620fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# We can now use this function for different repositories:\n",
    "    \n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa09c1-64d1-4b01-8308-c1aaf4a81345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_faq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3db9d-c3d6-41f6-8923-11f8e21435a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(evidently_docs[45]['content']) # 21712 \n",
    "# his is too long - we need to apply chunking say for 2k symbols here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ee661-7503-4386-9eab-81c8ef6377f0",
   "metadata": {},
   "source": [
    "## Observations during data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93ae96-c133-4535-9fd3-8da19cb9246e",
   "metadata": {},
   "source": [
    "Data Processing Considerations\n",
    "\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "\n",
    "Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "\n",
    "Performance: AI models work better with shorter text segments\n",
    "\n",
    "Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques below\n",
    "\n",
    "TODO\n",
    "\n",
    "-- Create a new uv project in the project directory\n",
    "\n",
    "-- Select a GitHub repo with documentation (preferably with .md files) - I can look for network security repos for example \n",
    "\n",
    "-- Download the data from there using the techniques we've learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd59df2-ecd7-4c24-a502-41ac731ab25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the document above at index 45 looks like:\n",
    "\n",
    "# {'title': 'LLM regression testing',\n",
    "#  'description': 'How to run regression testing for LLM outputs.',\n",
    "#  'content': 'In this tutorial, you will learn...'\n",
    "# }\n",
    "\n",
    "# The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. \n",
    "# For example, for size of 2000 characters, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 2000..4000\n",
    "# Chunk 3: 4000..6000\n",
    "\n",
    "# And so on.\n",
    "\n",
    "# However, this approach has disadvantages:\n",
    "\n",
    "# Context loss: Important information might be split in the middle\n",
    "# Incomplete sentences: Chunks might end mid-sentence\n",
    "# Missing connections: Related information might end up in different chunks\n",
    "\n",
    "# That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 1000..3000\n",
    "# Chunk 3: 2000..4000\n",
    "# ...\n",
    "\n",
    "# This is better for AI because:\n",
    "\n",
    "# Continuity: Important information isn't lost at chunk boundaries\n",
    "# Context preservation: Related sentences stay together in at least one chunk\n",
    "# Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "# This approach is known as the \"sliding window\" method. This is how we implement it in Python:\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "evidently_overlapping_chunks_45 = sliding_window(evidently_docs[45]['content'], 2000, 1000)\n",
    "evidently_overlapping_chunks_45[:2] # please note chunks are indeed OVERLAPPING below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712ffea-99fa-498c-832a-c47e83e30c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process all the documents in Evidently text dump:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e2a57-5b73-4a24-8ed4-a31edce38698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use copy() and pop() operations:\n",
    "\n",
    "# doc.copy() creates a shallow copy of the document dictionary\n",
    "# doc_copy.pop('content') removes the 'content' key and returns its value\n",
    "# This way we preserve the original dictionary keys that we can use later in the chunks.\n",
    "\n",
    "# This way, we obtain 575 chunks from 95 documents.\n",
    "\n",
    "# We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034564f-21c8-4cf6-8839-cf7167189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidently_chunks[:4]\n",
    "\n",
    "# There are some alternative approaches:\n",
    "\n",
    "# Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "# Advantages: More precise control over LLM input size\n",
    "# Disadvantages: Doesn't work well for documents with code\n",
    "# Paragraph splitting: Split by paragraphs\n",
    "# Section splitting: Split by sections\n",
    "# AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "# We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n",
    "\n",
    "# We will implement the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bd04f-0507-4436-98fa-978c04408ecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunks continued - Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03428c4-a9bb-4efa-b081-2c34fb17a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by paragraphs is relatively easy:\n",
    "\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "\n",
    "# We use \\n\\s*\\n regex pattern for splitting:\n",
    "\n",
    "# \\n matches a newline\n",
    "# \\s* matches zero or more whitespace characters\n",
    "# \\n matches another newline\n",
    "# So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
    "\n",
    "# This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n",
    "paragraphs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffa31e-bfa1-4505-85a4-c843dbabaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can combine sliding window and paragraph splitting for more intelligent processing. We won't do it here, but it's a good exercise to try.\n",
    "\n",
    "# Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# # Heading 1\n",
    "# ## Heading 2  \n",
    "# ### Heading 3\n",
    "\n",
    "# What we can do is split by headers.\n",
    "\n",
    "# For that we will use regex too:\n",
    "\n",
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. \n",
    "# But in general, this is not a big problem for documentation.\n",
    "\n",
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "# sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33119566-8e4a-41fd-995e-cf8d98a5288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. \n",
    "# This way, each section gets the same metadata (title, description) as the original document.\n",
    "\n",
    "# This was more intelligent processing, but we can go even further and use LLMs for that.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ef228-c811-413c-9984-d64fec8400e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b3ac9-9965-4ac5-9462-288ba1af53bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8b638-d6c7-4f73-b62d-e3f0090df74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "# This makes sense when:\n",
    "\n",
    "# Complex structure: Documents have complex, non-standard structure\n",
    "# Semantic coherence: You want chunks that are semantically meaningful\n",
    "# Custom logic: You need domain-specific splitting rules\n",
    "# Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "# This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "# Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "# You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "# You have complex, unstructured documents\n",
    "# Quality is more important than cost\n",
    "# You have the budget for LLM processing\n",
    "\n",
    "# Note: You can use any alternative LLM provider. One option is Groq, which is free with rate limits. You can replace the OpenAI library with the Groq library and it should work.\n",
    "\n",
    "# To continue, you need to get the API key from https://platform.openai.com/api-keys (assuming you have an account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf322203-9e0c-4f07-ba1b-ed5aee6cde44",
   "metadata": {},
   "source": [
    "Let's stop Jupyter and create an environment variable with your key:\n",
    "\n",
    "'''\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "'''\n",
    "\n",
    "Install the OpenAI SDK:\n",
    "\n",
    "'''\n",
    "uv add openai\n",
    "'''\n",
    "\n",
    "Then run jupyter notebook:\n",
    "\n",
    "'''\n",
    "uv run jupyter notebook\n",
    "'''\n",
    "\n",
    "It's cumbersome to set environment variables every time. I recommend using direnv, which works for Linux, Mac and Windows.\n",
    "\n",
    "Note: if you use direnv, don't forget to add .envrc to .gitignore.\n",
    "\n",
    "Warning: Never commit your API keys to git! Others can use your API key and you'll pay for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b52f5-fe7c-4c60-8afd-ae57ae471d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9ab5b-cf85-45c0-bd3b-6a3867abb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use OpenAI:\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "# This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results. \n",
    "# We will explain in more detail what this code does in the next lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db073911-134a-40b2-8942-5a861999cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a prompt:\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "# The prompt asks the LLM to:\n",
    "\n",
    "# Split the document logically (not just by length)\n",
    "# Make sections self-contained\n",
    "# Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2a34e-3e53-44db-9ed0-997d52cdc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function for intelligent chunking:\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0789621-74a4-4b36-8ed7-5a1bede6772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# tqdm is a library that shows progress bars. It helps you track progress when processing a large number of documents.\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. \n",
    "# For most applications, you don't need intelligent chunking.\n",
    "# this particular chunking took 30 mins and cost about US 5 cents or so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e8c0c-bf67-413f-b06d-cfbd43f3a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abc4a7-4cef-46a6-9e53-972cb1dcf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Choose a Chunking Approach\n",
    "# You may wonder - which chunking should I use? The answer: start with the simplest one and gradually increase complexity. \n",
    "# Start with simple chunking with overlaps. We will later talk about evaluations. \n",
    "# You can use evaluations to make informed decisions about chunking strategies.\n",
    "\n",
    "# Our data is ready. Now we can index it – insert it into a search engine and make it available for our (future) agent to use.\n",
    "\n",
    "# TODO:\n",
    "# For the project you selected, apply chunking\n",
    "# Experiment with simple chunking, paragraph chunking + sliding window, and section chunking\n",
    "# Which approach makes sense for your application? \n",
    "# Manually inspect the results and analyze what works best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970c75a-5b3c-4b3f-a4b8-ed723959d464",
   "metadata": {},
   "source": [
    "## Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82052e65-665c-4cef-b44e-006e29dd5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest type of search is a text search. Suppose we build a Q&A system for courses (using the FAQ dataset). \n",
    "# We want to find the answer to this question:\n",
    "# \"What should be in a test dataset for AI evaluation?\"\n",
    "# Text search works by finding all documents that contain at least one word from the query. \n",
    "# The more words from the query that appear in a document, the more relevant that document is.\n",
    "# This is how modern search systems like Apache Solr or Elasticsearch work. They use indexes to efficiently search through millions of documents without having to scan each one individually.\n",
    "# In this lesson, we'll start with a simple in-memory text search. The engine we will use is called minsearch.\n",
    "# Note: This search engine was implemented as part of a workshop I held some time ago. You can find details here if you want to know how it works.\n",
    "\n",
    "# Let's install it:\n",
    "# uv add minsearch\n",
    "\n",
    "# We will use it for chunked Evidently docs.\n",
    "# To remind you, this is how we prepared the docs:\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78a6680-34f0-4ae1-8a02-3dd301f42f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/r',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd9a72c-3990-4e91-b521-637966765a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x237b44aeba0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now index this data with minsearch:\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n",
    "# <minsearch.minsearch.Index at 0x237b44aeba0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a34ad9ff-0ca9-4770-943a-9c07f55f24a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>',\n",
       "  'title': 'RAG evaluation dataset',\n",
       "  'description': 'Synthetic data for RAG.',\n",
       "  'filename': 'docs-main/synthetic-data/rag_data.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"Who painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  -',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 8000,\n",
       "  'chunk': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 5000,\n",
       "  'chunk': 'he notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a set of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", ',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nEvidently helps you evaluate LLM outputs automatically. The lets you compare prompts, models, run regression or adversarial tests with clear, repeatable checks. That means faster iterations, more confident decisions, and fewer surprises in production.\\n\\nIn this Quickstart, you\\'ll try a simple eval in Python and view the results in Evidently Cloud. If you want to stay fully local, you can also do that - just skip a couple steps.\\n\\nThere are a few extras, like custom LLM judges or tests, if you want to go further.\\n\\nLet’s dive in.\\n\\n<Info>\\n  Need help at any point? Ask on [Discord](https://discord.com/invite/xZjKRaNp8b).\\n</Info>\\n\\n## 1. Set up your environment\\n\\nFor a fully local flow, skip steps 1.1 and 1.3.\\n\\n### 1.1. Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nComponents to run the evals:\\n\\n```python\\nimport pandas as pd\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently.presets import TextEvals\\nfrom evidently.tests import lte, gte, eq\\nfrom evidently.descriptors import LLMEval, TestSummary, DeclineLLMEval, Sentiment, TextLength, IncludesWords\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nComponents to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />\\n\\n## 2. Prepare the dataset\\n\\nLet\\'s create a toy demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    [\"What is the capital of Japan?\", \"The capital of Japan is Tokyo.\"],\\n    [\"Tell me a joke.\", \"Why don\\'t programmers like nature? Too many bugs!\"],\\n    [\"When does water boil?\", \"Water\\'s boiling point is 100 degrees Celsius.\"],\\n    [\"W',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 6000,\n",
       "  'chunk': 'et of panels with descriptor values. Each will have a single data point for now. As you log more evaluation results, you can track trends and set up alerts.\\n\\nWant to see more complex workflows? You can add pass/fail conditions and custom evals.\\n\\n## 6. (Optional) Add tests\\n\\nYou can add conditions to your evaluations. For example, you may expect that:\\n\\n- **Sentiment** is non-negative (greater or equal to 0)\\n- **Text length** is at most 150 symbols (less or equal to 150).\\n- **Denials**: there are none.\\n- If any condition is false, consider the output to be a \"fail\".\\n\\nYou can implement this logic easily.\\n\\n<Accordion title=\"Add test conditions\" description=\"How to add test conditions\" icon=\"ballot-check\">\\n  ```python\\n  # Run the evaluation with tests \\n  eval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\",\\n                  tests=[gte(0, alias=\"Is_non_negative\")]),\\n        TextLength(\"answer\", alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-i',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'on=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in the notebook cell. You can also get a JSON or Python dictionary, or save as an external HTML file.\\n\\n```python\\n# my_eval.json()\\n# my_eval.dict()\\n# my_report.save_html(“file.html”)\\n```\\n\\nLocal Reports are great for quick experiments. To run comparisons, keep track of the results and collaborate with others, upload the results to Evidently Platform.\\n\\n**Upload the Report to Evidently Cloud** together with scored data:\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\n**Explore.** Go to [Evidently Cloud](https://app.evidently.cloud/), open your Project, and navigate to Reports. You will see all score summaries and can browse the data. E.g. sort to find all answers labeled as \"Denials\".\\n\\n![](/images/examples/llm_quickstart_explore.png)\\n\\n## 5. Get a Dashboard \\n\\nAs you run more evals, it\\'s useful to track them over time. Go to \"Dashboard\" in the left menu, enter the \"Edit\" mode, and add a new \"Columns\" tab:\\n\\n![](/images/examples/llm_quickstart_create_tab_new.gif)\\n\\nYou\\'ll see a s',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 7000,\n",
       "  'chunk': 'alias=\"Length\",\\n                   tests=[lte(150, alias=\"Has_expected_length\")]),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\",\\n                       tests=[eq(\"OK\", column=\"Denials\",\\n                                 alias=\"Is_not_a_refusal\")]),\\n        TestSummary(success_all=True, alias=\"All_tests_passed\")])\\n  \\n  # Uncomment to preview the results locally\\n  # eval_dataset.as_dataframe()\\n  ```\\n\\n  ![](/images/examples/llm_quickstart_descriptor_tests-min.png)\\n\\n  You can limit the summary report to include only specific descriptor(s).\\n\\n  ```python\\n  report = Report([\\n      TextEvals(columns=[\"All_tests_passed\"])\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  #my_eval\\n  ```\\n\\n  To identify rows that failed any criteria, sort by \"All_test_passed\" column:\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_tests_report-min.png)\\n\\n## 7. (Optional) Add a custom LLM jugde\\n\\nYou can implement custom criteria using built-in LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Ru',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we create an index that will search through four text fields: chunk content, title, description, and filename. The keyword_fields parameter is for exact matches (we don't need it for now).\n",
    "# We can now use it for search:\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n",
    "results\n",
    "\n",
    "# This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9464c6a-e51a-4fcf-a35c-93b9aacc2cb2",
   "metadata": {},
   "source": [
    "## Vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f12725-7dde-4922-91a1-a12e9c22bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text search has limitations. Consider these two queries:\n",
    "# \"I just discovered the program, can I still enroll?\"\n",
    "# \"I just found out about the course, can I still join?\"\n",
    "# These ask the same question but share no common words (among important ones). Text search would fail to find relevant matches.\n",
    "# This is where embeddings help. Embeddings are numerical representations of text that capture semantic meaning. \n",
    "# Words and phrases with similar meanings have similar embeddings, even if they use different words.\n",
    "# Vector search uses these embeddings to identify semantically similar documents, rather than just exact word matches.\n",
    "# For vector search, we need to turn our documents into vectors (embeddings).\n",
    "# We will use the sentence-transformers library for this purpose.\n",
    "\n",
    "# Install it:\n",
    "# uv add sentence-transformers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
