{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1cac94-1f14-44f9-a8bc-1c359b395889",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download data and install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17d7bfa-3616-4e22-b579-35b48511c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd76b1-7566-4643-b71c-495a3e5f7a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download the repository as a zip file. GitHub provides a convenient URL format for this:\n",
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n",
    "# NB!! This code downloads the file to memory, not to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb396e-1759-4995-8203-bc5e4fefbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e3e28-5a9c-4a25-b2de-eda7c108e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fe6486-4ce8-4346-9963-2b0e4f12faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Implementation\n",
    "# Let's now put everything together into a reusable function:\n",
    "\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ba8192-69c9-4667-9a4c-ae558620fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "# We can now use this function for different repositories:\n",
    "    \n",
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa09c1-64d1-4b01-8308-c1aaf4a81345",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_faq[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da3db9d-c3d6-41f6-8923-11f8e21435a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(evidently_docs[45]['content']) # 21712 \n",
    "# his is too long - we need to apply chunking say for 2k symbols here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ee661-7503-4386-9eab-81c8ef6377f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Observations during data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93ae96-c133-4535-9fd3-8da19cb9246e",
   "metadata": {},
   "source": [
    "Data Processing Considerations\n",
    "\n",
    "For FAQ, the data is ready to use. These are small records that we can index (put into a search engine) as is.\n",
    "For Evidently docs, the documents are very large. We need extra processing called \"chunking\" - breaking large documents into smaller, manageable pieces. This is important because:\n",
    "\n",
    "Search relevance: Smaller chunks are more specific and relevant to user queries\n",
    "\n",
    "Performance: AI models work better with shorter text segments\n",
    "\n",
    "Memory limits: Large documents might exceed token limits of language models\n",
    "\n",
    "We will cover chunking techniques below\n",
    "\n",
    "TODO\n",
    "\n",
    "-- Create a new uv project in the project directory\n",
    "\n",
    "-- Select a GitHub repo with documentation (preferably with .md files) - I can look for network security repos for example \n",
    "\n",
    "-- Download the data from there using the techniques we've learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd59df2-ecd7-4c24-a502-41ac731ab25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\"},\n",
       " {'start': 1000,\n",
       "  'chunk': \".\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.future.datasets import Descriptor\\nfrom evidently.future.descriptors import *\\nfrom evidently.future.report import Report\\nfrom evidently.future.presets import TextEvals\\nfrom evidently.future.metrics import *\\nfrom evidently.future.tests import *\\n\\nfrom evidently.features.llm_judge import BinaryClassificationPromptTemplate\\n```\\n\\nTo connect to Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace.cloud import CloudWorkspace\\n```\\n\\n**Optional.** To create monitoring panels as code:\\n\\n```python\\nfrom evidently.ui.dashboards import DashboardPanelPlot\\nfrom evidently.ui.dashboards import DashboardPanelTestSuite\\nfrom evidently.ui.dashboards import DashboardPanelTestSuiteCounter\\nfrom evidently.ui.dashboards import TestSuitePanelType\\nfrom evidently.ui.dashboards import ReportFilter\\nfrom evidently.ui.dashboards import PanelValue\\nfrom evidently.ui.dashboards import PlotType\\nfrom evidently.ui.dashboards import CounterAgg\\nfrom evidently.tests.base_test import TestStatus\\nfrom evidently.renderers.html_widgets imp\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how the document above at index 45 looks like:\n",
    "\n",
    "# {'title': 'LLM regression testing',\n",
    "#  'description': 'How to run regression testing for LLM outputs.',\n",
    "#  'content': 'In this tutorial, you will learn...'\n",
    "# }\n",
    "\n",
    "# The content field is 21,712 characters long. The simplest thing we can do is cut it into pieces of equal length. \n",
    "# For example, for size of 2000 characters, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 2000..4000\n",
    "# Chunk 3: 4000..6000\n",
    "\n",
    "# And so on.\n",
    "\n",
    "# However, this approach has disadvantages:\n",
    "\n",
    "# Context loss: Important information might be split in the middle\n",
    "# Incomplete sentences: Chunks might end mid-sentence\n",
    "# Missing connections: Related information might end up in different chunks\n",
    "\n",
    "# That's why, in practice, we usually make sure there's overlap between chunks. For size 2000 and overlap 1000, we will have:\n",
    "\n",
    "# Chunk 1: 0..2000\n",
    "# Chunk 2: 1000..3000\n",
    "# Chunk 3: 2000..4000\n",
    "# ...\n",
    "\n",
    "# This is better for AI because:\n",
    "\n",
    "# Continuity: Important information isn't lost at chunk boundaries\n",
    "# Context preservation: Related sentences stay together in at least one chunk\n",
    "# Better search: Queries can match information even if it spans chunk boundaries\n",
    "\n",
    "# This approach is known as the \"sliding window\" method. This is how we implement it in Python:\n",
    "\n",
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "evidently_overlapping_chunks_45 = sliding_window(evidently_docs[45]['content'], 2000, 1000)\n",
    "evidently_overlapping_chunks_45[:2] # please note chunks are indeed OVERLAPPING below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712ffea-99fa-498c-832a-c47e83e30c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's process all the documents in Evidently text dump:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2e2a57-5b73-4a24-8ed4-a31edce38698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use copy() and pop() operations:\n",
    "\n",
    "# doc.copy() creates a shallow copy of the document dictionary\n",
    "# doc_copy.pop('content') removes the 'content' key and returns its value\n",
    "# This way we preserve the original dictionary keys that we can use later in the chunks.\n",
    "\n",
    "# This way, we obtain 575 chunks from 95 documents.\n",
    "\n",
    "# We can play with the parameters by including more or less content. 2000 characters is usually good enough for RAG applications.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034564f-21c8-4cf6-8839-cf7167189b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidently_chunks[:4]\n",
    "\n",
    "# There are some alternative approaches:\n",
    "\n",
    "# Token-based chunking: You first tokenize the content (turn it into a sequence of words) and then do a sliding window over tokens\n",
    "# Advantages: More precise control over LLM input size\n",
    "# Disadvantages: Doesn't work well for documents with code\n",
    "# Paragraph splitting: Split by paragraphs\n",
    "# Section splitting: Split by sections\n",
    "# AI-powered splitting: Let AI split the text intelligently\n",
    "\n",
    "# We won't cover token-based chunking here, as we're working with documents that contain code. But it's easy to implement - ask ChatGPT for help if you need it for text-only content.\n",
    "\n",
    "# We will implement the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758bd04f-0507-4436-98fa-978c04408ecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunks continued - Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03428c4-a9bb-4efa-b081-2c34fb17a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting by paragraphs is relatively easy:\n",
    "\n",
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "\n",
    "# We use \\n\\s*\\n regex pattern for splitting:\n",
    "\n",
    "# \\n matches a newline\n",
    "# \\s* matches zero or more whitespace characters\n",
    "# \\n matches another newline\n",
    "# So \\n\\s*\\n matches two newlines with optional whitespace between them\n",
    "\n",
    "# This works well for literature, but it doesn't work well for documents. Most paragraphs in technical documentation are very short.\n",
    "paragraphs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffa31e-bfa1-4505-85a4-c843dbabaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can combine sliding window and paragraph splitting for more intelligent processing. We won't do it here, but it's a good exercise to try.\n",
    "\n",
    "# Let's now look at section splitting. Here, we take advantage of the documents' structure. Markdown documents have this structure:\n",
    "\n",
    "# # Heading 1\n",
    "# ## Heading 2  \n",
    "# ### Heading 3\n",
    "\n",
    "# What we can do is split by headers.\n",
    "\n",
    "# For that we will use regex too:\n",
    "\n",
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Note: This code may not work perfectly if we want to split by level 1 headings and have Python code with # comments. \n",
    "# But in general, this is not a big problem for documentation.\n",
    "\n",
    "# If we want to split by second-level headers, that's what we do:\n",
    "\n",
    "# sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33119566-8e4a-41fd-995e-cf8d98a5288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we iterate over all the docs to create the final result:\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# Like previously, copy() creates a copy of the document metadata. pop('content') removes and returns the content. \n",
    "# This way, each section gets the same metadata (title, description) as the original document.\n",
    "\n",
    "# This was more intelligent processing, but we can go even further and use LLMs for that.\n",
    "len(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ef228-c811-413c-9984-d64fec8400e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b3ac9-9965-4ac5-9462-288ba1af53bb",
   "metadata": {},
   "source": [
    "## Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8b638-d6c7-4f73-b62d-e3f0090df74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In some cases, we want to be more intelligent with chunking. Instead of doing simple splits, we delegate this work to AI.\n",
    "\n",
    "# This makes sense when:\n",
    "\n",
    "# Complex structure: Documents have complex, non-standard structure\n",
    "# Semantic coherence: You want chunks that are semantically meaningful\n",
    "# Custom logic: You need domain-specific splitting rules\n",
    "# Quality over cost: You prioritize quality over processing cost\n",
    "\n",
    "# This costs money. In most cases, we don't need intelligent chunking.\n",
    "\n",
    "# Simple approaches are sufficient. Use intelligent chunking only when\n",
    "\n",
    "# You already evaluated simpler methods and you can confirm that they produce poor results\n",
    "# You have complex, unstructured documents\n",
    "# Quality is more important than cost\n",
    "# You have the budget for LLM processing\n",
    "\n",
    "# Note: You can use any alternative LLM provider. One option is Groq, which is free with rate limits. You can replace the OpenAI library with the Groq library and it should work.\n",
    "\n",
    "# To continue, you need to get the API key from https://platform.openai.com/api-keys (assuming you have an account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf322203-9e0c-4f07-ba1b-ed5aee6cde44",
   "metadata": {},
   "source": [
    "Let's stop Jupyter and create an environment variable with your key:\n",
    "\n",
    "'''\n",
    "export OPENAI_API_KEY='your-api-key'\n",
    "'''\n",
    "\n",
    "Install the OpenAI SDK:\n",
    "\n",
    "'''\n",
    "uv add openai\n",
    "'''\n",
    "\n",
    "Then run jupyter notebook:\n",
    "\n",
    "'''\n",
    "uv run jupyter notebook\n",
    "'''\n",
    "\n",
    "It's cumbersome to set environment variables every time. I recommend using direnv, which works for Linux, Mac and Windows.\n",
    "\n",
    "Note: if you use direnv, don't forget to add .envrc to .gitignore.\n",
    "\n",
    "Warning: Never commit your API keys to git! Others can use your API key and you'll pay for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9b52f5-fe7c-4c60-8afd-ae57ae471d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9ab5b-cf85-45c0-bd3b-6a3867abb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're ready to use OpenAI:\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text\n",
    "\n",
    "# This code invokes an LLM (gpt-4o-mini) with the provided prompt and returns the results. \n",
    "# We will explain in more detail what this code does in the next lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db073911-134a-40b2-8942-5a861999cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a prompt:\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "# The prompt asks the LLM to:\n",
    "\n",
    "# Split the document logically (not just by length)\n",
    "# Make sections self-contained\n",
    "# Use a specific output format that's easy to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2a34e-3e53-44db-9ed0-997d52cdc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function for intelligent chunking:\n",
    "\n",
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0789621-74a4-4b36-8ed7-5a1bede6772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply this to every document:\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)\n",
    "\n",
    "# tqdm is a library that shows progress bars. It helps you track progress when processing a large number of documents.\n",
    "\n",
    "# Note: This process requires time and incurs costs. As mentioned before, use this only when really necessary. \n",
    "# For most applications, you don't need intelligent chunking.\n",
    "# this particular chunking took 30 mins and cost about US 5 cents or so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e8c0c-bf67-413f-b06d-cfbd43f3a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abc4a7-4cef-46a6-9e53-972cb1dcf467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Choose a Chunking Approach\n",
    "# You may wonder - which chunking should I use? The answer: start with the simplest one and gradually increase complexity. \n",
    "# Start with simple chunking with overlaps. We will later talk about evaluations. \n",
    "# You can use evaluations to make informed decisions about chunking strategies.\n",
    "\n",
    "# Our data is ready. Now we can index it – insert it into a search engine and make it available for our (future) agent to use.\n",
    "\n",
    "# TODO:\n",
    "# For the project you selected, apply chunking\n",
    "# Experiment with simple chunking, paragraph chunking + sliding window, and section chunking\n",
    "# Which approach makes sense for your application? \n",
    "# Manually inspect the results and analyze what works best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970c75a-5b3c-4b3f-a4b8-ed723959d464",
   "metadata": {},
   "source": [
    "## Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82052e65-665c-4cef-b44e-006e29dd5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest type of search is a text search. Suppose we build a Q&A system for courses (using the FAQ dataset). \n",
    "# We want to find the answer to this question:\n",
    "# \"What should be in a test dataset for AI evaluation?\"\n",
    "# Text search works by finding all documents that contain at least one word from the query. \n",
    "# The more words from the query that appear in a document, the more relevant that document is.\n",
    "# This is how modern search systems like Apache Solr or Elasticsearch work. They use indexes to efficiently search through millions of documents without having to scan each one individually.\n",
    "# In this lesson, we'll start with a simple in-memory text search. The engine we will use is called minsearch.\n",
    "# Note: This search engine was implemented as part of a workshop I held some time ago. You can find details here if you want to know how it works.\n",
    "\n",
    "# Let's install it:\n",
    "# uv add minsearch\n",
    "\n",
    "# We will use it for chunked Evidently docs.\n",
    "# To remind you, this is how we prepared the docs:\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78a6680-34f0-4ae1-8a02-3dd301f42f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'docs-main/api-reference/introduction.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/r',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'docs-main/changelog/changelog.mdx'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd9a72c-3990-4e91-b521-637966765a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x237b44aeba0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's now index this data with minsearch:\n",
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n",
    "# <minsearch.minsearch.Index at 0x237b44aeba0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a34ad9ff-0ca9-4770-943a-9c07f55f24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create an index that will search through four text fields: chunk content, title, description, and filename. The keyword_fields parameter is for exact matches (we don't need it for now).\n",
    "# We can now use it for search:\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n",
    "# results\n",
    "\n",
    "# This is text search, also known as \"lexical search\". We look for exact matches between our query and the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9464c6a-e51a-4fcf-a35c-93b9aacc2cb2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f12725-7dde-4922-91a1-a12e9c22bf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8793c116d2f24c0e8e0c4ab8c116eba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tmp\\aihero\\course\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Denis\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-distilbert-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31c7c04883349a1a50ae40074c17c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64462c2d5e5f48faa7b882b0439884f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327beecbb087422fb3a2b8ad275967f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01c93153f0b4a229127cfabcd1907ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/523 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9587f90ed83f42768c6b3f0d6a02875d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259d9bfac6684d3c97b5bf3e9678338e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dead63ca254744b2d8dbd7fa6de723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0410705e5ca43838c2028aa04ebf48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d18502984704e41835f048630a02cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91640fa4ed8c4354900ac34d7e4d86ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text search has limitations. Consider these two queries:\n",
    "# \"I just discovered the program, can I still enroll?\"\n",
    "# \"I just found out about the course, can I still join?\"\n",
    "# These ask the same question but share no common words (among important ones). Text search would fail to find relevant matches.\n",
    "# This is where embeddings help. Embeddings are numerical representations of text that capture semantic meaning. \n",
    "# Words and phrases with similar meanings have similar embeddings, even if they use different words.\n",
    "# Vector search uses these embeddings to identify semantically similar documents, rather than just exact word matches.\n",
    "# For vector search, we need to turn our documents into vectors (embeddings).\n",
    "# We will use the sentence-transformers library for this purpose.\n",
    "\n",
    "# Install it:\n",
    "# uv add sentence-transformers\n",
    "\n",
    "# Let's use it:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "\n",
    "# The multi-qa-distilbert-cos-v1 model is trained explicitly for question-answering tasks. \n",
    "# It creates embeddings optimized for finding answers to questions.\n",
    "# Other popular models include:\n",
    "# all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
    "# all-mpnet-base-v2 - Higher quality, slower\n",
    "# Check Sentence Transformers documentation for more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151f740c-ce50-46c1-8bcc-53af04791d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6b894c40eb4a38a01b7822e9694325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x237c2c547a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can easily do the same with the Evidently docs (but only use the chunk field for embeddings):\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from minsearch import VectorSearch\n",
    "\n",
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n",
    "\n",
    "# <minsearch.vector.VectorSearch at 0x237c2c547a0>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8931bdc1-d361-4348-800a-0d5b74f4b191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\\n\\n* Just the inputs, or\\n* Both inputs and expected outputs (ground truth).\\n\\nYou can use this test dataset to:\\n\\n* Run **experiments** and track if changes improve or degrade system performance.\\n* Run **regression testing** to ensure updates don’t break what was already working.\\n* **Stress-test** your system with complex or adversarial inputs to check its resilience.\\n\\n![](/images/synthetic/synthetic_experiments_img.png)\\n\\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don’t reflect your specific use case. Manually writing test cases takes time and effort.\\n\\n**Synthetic data helps here**. It’s especially useful when you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. I\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \" you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios.\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"a set of Tests to evaluate your data or AI system. Each Report Preset has this option. \\n\\nEnable it by setting `include_tests=True` on the Report level. (Default: False).\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n],\\ninclude_tests=True)\\n```\\n\\nFor example, while the `DataSummaryPreset()` Report simply shows descriptive stats of your data, adding the Tests will additionally run multiple checks on data quality and expected column statistics.\\n\\nThe automatic Test conditions can either\\n* be derived from a reference dataset, or\\n* use built-in heuristics.\\n\\n**Using reference**. When you provide a reference dataset, Tests compare the new data against it:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, eval_data_2) # eval_data_2 is reference\\n```\\n\\nFor example, the check on missing values will validate if the current share of missing values is within +/-10% of the reference.\\n\\n<Note>\\nNote that in this case the order matters: the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline and use to generate test conditions.\\n</Note>\\n\\n**Using heuristics**. Without reference, Tests use predefined rules:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, None) # no reference data\\n```\\n\\nIn this case, the missing values Test simply expects 0% missing values. Similarly, classification accuracy Test will compare the performance against a dummy model, etc. Some metrics (like min/max/mean values) don't have default heuristics.\\n\\n<Info>\\n  **How to check Test defaults?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n### Individual Tests with defaults\\n\\nPresets are great for a start or quick sanity checks, but often you'd want to select specific Tests. For example, instead of running checks on all value statistics, validate only mean or max.\\n\\nYou can pick the Tests while still using default conditions.\\n\\n**Select Tests**. List the individual Metrics, and choose the the `include_Tests` option:\\n\\n```Py\",\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': \"se cases, Evidently also helps you generate synthetic test datasets - such as RAG-style question-answer pairs from a knowledge base or synthetic inputs to cold-start your AI app testing.\\n\\n**📌 Links:** \\n- [Synthetic data](docs/library/synthetic_data_api) \\n\\n**3. Prompt optimization [NEW]**\\n\\n<Check>\\n  **TL;DR**: We help write prompts using labeled or annotated data as a target.\\n</Check>\\n\\nEvidently also includes tools for automated prompt writing. This features uses built-in evaluation capabilities to score prompt variations, optimizing them based on a target dataset and/or free-form user feedback.\\n\\nThis feature also help automatically generate LLM judge prompts to streamline the creation of custom evaluations.\\n\\n**📌 Links:** \\n- [Prompt optimization](docs/library/prompt_optimization)\\n\\n4. **Tracking and Visualization UI**\\n\\n<Check>\\n  **TL;DR**: There is also a minimal UI to store and track evaluation results.\\n</Check>\\n\\nThe Evidently library also includes a lightweight self-hostable UI for storing, comparing, and visualizing evaluation results over time.\\n\\nWhile visual reports provide a snapshot of an evaluation for a specific period, dataset, or prompt version, the UI allows you to store multiple evaluations and track changes over time.\\n\\n![](/images/concepts/evidently_oss_ui-min.png)\\n\\n**📌 Links:**\\n\\n- See live demo: [https://demo.evidentlyai.com](https://demo.evidentlyai.com/). \\n- [Self-hosting guide](/docs/setup/self-hosting)\\n\\n<Note>\\n  The open-source UI is different from the Evidently Cloud / Enterprise platform version which has muliple additional features. Explore the [Evidently Platform capabailities](/docs/platform/overview). \\n</Note>\\n\\n# Core evaluation concepts\\n\\nLet's take a look at the end-to-end evaluation process. It can be adapted to different metrics or data types, following the same worklows. \\n\\n## Dataset\\n\\nTo run an evaluation, you first need to prepare the data. For example, generate and trace outputs from your ML or LLM system.\\n\\n1. **Prepare your data as a pan\",\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"You may need evaluations at different stages of your AI product development:\\n\\n* **Ad hoc analysis.** Spot-check the quality of your data or AI outputs.\\n\\n* **Experiments**. Test different parameters, models, or prompts and compare outcomes.\\n\\n* **Safety and adversarial testing.** Evaluate how your system handles edge cases and adversarial inputs, including on synthetic data.\\n\\n* **Regression testing.** Ensure the performance does not degrade after updates or fixes.\\n\\n* **Monitoring**. Track the response quality for production systems.\\n\\nEvidently supports all these workflows. You can run evals locally or directly on the platform.\\n\\n## Evaluations via API\\n\\n<Check>\\n  Supported in: `Evidently OSS`, `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis is perfect for experiments, CI/CD workflows, or custom evaluation pipelines.\\n\\n![](/images/evals_flow_python.png)\\n\\n**How it works**:\\n\\n* Run Python-based evaluations on your AI outputs by generating Reports.\\n\\n* Upload results to the Evidently Platform.\\n\\n* Use the Explore feature to compare and debug results between runs.\\n\\n**Next step:** check the Quickstart for [ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n\\n## No-code evaluations\\n\\n<Check>\\n  Supported in `Evidently Cloud` and `Evidently Enterprise`.\\n</Check>\\n\\nThis option lets you run evaluations directly in the user interface. This is great for non-technical users or when you prefer to run evaluations on Evidently infrastructure.\\n\\n![](/images/evals_flow_nocode.png)\\n\\n**How it works**:\\n\\n* **Analyze CSV datasets**. Drag and drop CSV files and evaluate their contents on the Platform.\\n\\n* **Evaluate uploaded datasets**. Assess collected [traces](/docs/platform/tracing_overview) from instrumented LLM applications or any [Datasets](/docs/platform/datasets_overview) you previously uploaded or generated.\\n\\nNo-code workflows create the same Reports or Test Suites you'd generate using Python. The rest of the workflow is the same. After you run your evals with any method, you can acces\",\n",
       "  'title': 'Overview',\n",
       "  'description': 'Running evals on the platform.',\n",
       "  'filename': 'docs-main/docs/platform/evals_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': ' to the \"Dataset\" page in your Project menu. For raw tracing datasets, check the Tracing section.\\n</Tip>\\n\\n## Synthetic Data\\n\\nYou can synthesize evaluation datasets directly in Evidently Platform:\\n\\n* **Generate from examples or description**. Describe specific test scenarios and generate matching datasets.\\n\\n* **Generate from source documents**. Generate Q\\\\&A pairs from source documents like PDF, CSV or markdown files (great for RAG evaluations).\\n\\nAfter creating or uploading datasets, you can edit or diversify them further using the \"more like this\" feature.\\n\\n## When do you need Datasets?\\n\\nHere are common use cases for datasets in Evidently:\\n\\n* **Organize evaluation datasets**. Save curated datasets with expected inputs and optional ground truth outputs. You can bring in domain experts to collaborate on these datasets in UI, and access them programmatically for CI/CD checks.\\n\\n* **Debug evaluation results**. After you run an evaluation, view the dataset to identify and debug specific failures. E.g. you can sort all text outputs by added scores.\\n\\n* **Store ML inference logs or LLM traces**. Collect raw data from production or experimental runs, use it as a source of truth, and run evaluations over it.',\n",
       "  'title': 'Overview',\n",
       "  'description': 'Introduction to Datasets.',\n",
       "  'filename': 'docs-main/docs/platform/datasets_overview.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': 'n the column type (numerical, categorical, text, datetime).\\n\\n**Test suite**. If you choose to enable Tests, you will get an additional Test Suite view:\\n\\n![](/images/metrics/test_preset_dataset_summary-min.png)\\n\\nTests are auto-generated:\\n\\n* **Based on reference dataset.** If the reference dataset is provided, conditions like min-max feature ranges are derived directly from it.\\n\\n* **Based on heuristics.** If there is no reference, some Tests will run with heuristics (like expect no missing values).\\n\\n<Info>\\n  **How Tests work.** Read about [Tests](/docs/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nYou can use this Preset in different scenarios.\\n\\n* **Exploratory data analysis.** Use the visual Report to explore your dataset at any point (during model training, after new batch of data arrives, during debugging etc.)\\n\\n* **Dataset comparison.** Compare any datasets to understand the differences: training and test dataset, subgroups in the same dataset, current production data against training, etc..\\n\\n* **Data quality tests in production.** By enabling Tests, you can check the quality and stability of the input data before you generate the predictions, every time you perform a certain transformation, add a new data source, etc.\\n\\n* **Data profiling in production.** You can use this preset during monitoring to capture the shape of the production data for future analysis and visualization.\\n\\n## Data requirements\\n\\n* **Input columns**. You can provide any input columns. They must be non-empty.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Set column types.** The Preset evaluates numerical, categorical, text and DateTime columns. You can specify column types explicitly (recommended). Otherwise Evidently will auto-detect numerical, categorical and datetime columns. You must always map text data.\\n\\n<Info>\\n  **Data schema mapping**. Use the [data def',\n",
       "  'title': 'Data Summary',\n",
       "  'description': 'Overview of the Data Summary Preset.',\n",
       "  'filename': 'docs-main/metrics/preset_data_summary.mdx'},\n",
       " {'start': 14000,\n",
       "  'chunk': ' across all inputs\\n- analyze any tabular dataset (descriptive stats, quality, drift)\\n- evaluate AI system performance (regression, classification, ranking, etc.)\\n\\nEach Report runs a computation and visualizes a set of **Metrics** and conditional **Tests.** If you pass two datasets, you get a side-by-side comparison.\\xa0\\n\\nThe easiest way to start is by using **Presets**.\\n\\n### Metric Presets\\n\\nPresets are pre-configured evaluation templates.\\n\\nThey help compute multiple related Metrics using a single line of code. Evidently has a number of **comprehensive Presets** ([see all](/metrics/all_presets)) for specific evaluation scenarios: from exploratory data analysis to AI quality assessments. For example:\\n\\n<Tabs>\\n  <Tab title=\"TextEvals\">\\n    `TextEvals` summarizes the scores from all text descriptors.\\n\\n    ![](/images/examples/llm_quickstart_report.png)\\n  </Tab>\\n  <Tab title=\"Data Drift\">\\n    `DataDriftPreset` identifies shifts in data distribution for all dataset columns.\\n\\n    ![](/images/concepts/overview_drift_report-min.png)\\n  </Tab>\\n  <Tab title=\"Data Summary\">\\n    `DataSummaryPreset` summarizes all dataset columns, generating statistics and profiles for each.\\n\\n    ![](/images/metrics/preset_datasummary_example-min.png)\\n  </Tab>\\n  <Tab title=\"Classification\">\\n    `ClassificationPreset` breaks down classification metrics and includes debugging plots.\\n\\n    ![](/images/metrics/preset_classification_example-min.png)\\n  </Tab>\\n</Tabs>\\n\\n### Metrics\\n\\nEach Preset is made of individual Metrics. You can also create your own **custom Report** by listing the `Metrics` you want to include.\\n\\n- You can combine multiple Metrics and Presets in a Report.\\xa0\\n- You can include both built-in Metrics and custom Metrics.\\n\\nBuilt-in Metrics range from simple statistics like `MeanValue` or `MissingValueCount` to complex algorithmic evals like `DriftedColumnsCount`.\\n\\nEach **Metric** computes a single value and has an optional visual representation (or several to choose from). For convenience, there ',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Core concepts and components of the Evidently Python library.',\n",
       "  'filename': 'docs-main/docs/library/overview.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"We’re building Evidently AI to help teams ship reliable AI products: whether it’s an ML model, an LLM app, or a complex agent workflow.\\n\\nOur tools are model-, framework-, and application-agnostic, so you can build and evaluate AI systems your way without limitations.\\n\\n## We are open-source\\n\\n[**Evidently**](https://github.com/evidentlyai/evidently) is an open-source library with over 25 million downloads, 5000+ GitHub stars, and a thriving community. It's licensed under Apache 2.0. This gives full transparency - you can see exactly how every metric works and trust the implementation. It also delivers an intuitive API designed for a great developer experience.\\n\\nThe **Evidently Platform** builds on the library with additional UI features and workflows for team collaboration. For enterprise users, we offer both Cloud and self-hosted options for full data privacy and control.\\n\\n## Evidently is very modular\\xa0\\n\\nEvidently is built to adapt to your needs without lock-ins or complex setups. It’s modular and component-based, so you can start small: you don't have to deploy a service with multiple databases just to run a single eval.\\n\\n* Start with local ad hoc checks.\\xa0\\n\\n* Want to share results? Add a UI to track evaluations over time.\\xa0\\n\\n* When you run evals, choose to upload raw data or only evaluation results. It’s up to you.\\xa0\\n\\n* Add monitoring as you are ready to move to production workflows.\\n\\nEvidently is built around the concept of **Presets** and **reasonable defaults**: you can run any evaluation with minimal setup, including with auto-generated test conditions for assertions. \\n\\nEvidently also integrates with your existing tools and lets you easily export metrics, reports, and datasets elsewhere.\\xa0\\n\\n## 100+ built-in evaluations\\n\\nEvidently puts evaluations and quality testing first.\\xa0\\n\\nMany other tools provide a system to run and log evals, but expect you to prepare the data and implement all the metrics from scratch. We ship **100+ built-in evaluations** that cover many ML an\",\n",
       "  'title': 'Why Evidently?',\n",
       "  'description': 'Why choose Evidently.',\n",
       "  'filename': 'docs-main/faq/why_evidently.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 's/library/tests) and check defaults for each Test in the [reference table.](/metrics/all_metrics)\\n</Info>\\n\\n## Use case\\n\\nThese Presets are useful in various scenarios:\\n\\n* **Model / system comparison**. Compare predictive system performance across different datasets, such as during A/B testing, when experimenting with different prompt versions and configurations, etc.\\n\\n* **Production monitoring**. You can run evaluations whenever you get true labels in production. Use this to communicate and visualize performance, decide on model updates / retraining, etc.\\n\\n* **Debugging**. If you notice a drop in performance, use the visual Report\\n  Model Monitoring: Track the performance of a classification model over time to diagnose quality issues, explore the model errors and underperforming segments.\\n\\n## Data requirements\\n\\n* **Target and prediction columns**. Required to calculate performance.\\n\\n* **One or two datasets**. Pass two for a side-by-side comparison or to auto-generate tests.\\n\\n* (Optional) **Input features.** Include if you want to explore column-target relations.\\n\\n* (Optional) **Timestamp**. If available, pass it to appear on some plots.\\n\\n<Info>\\n  **Data schema mapping.** Use the [data definition](/docs/library/data_definition) to map your data structure.\\n</Info>\\n\\n## Report Customization\\n\\nYou can customize the Report in several ways:\\n\\n* **Change Test conditions**. To modify the auto-generated conditions, you can set yours: either a different condition relative to the reference or any custom conditions.\\n\\n* **Modify Report composition**. You can add additional metrics, such as column Correlations, Missing Values, or Data Drift. It\\'s often useful to add `ValueDrift(\"target\")`to evaluate if there is a statistical distribution shift in the model target (concept drift).\\n\\n<Info>\\n  **Creating a custom Report**. Check the documentation for creating a [custom Report](/docs/library/report) and modifying [Tests](/docs/library/tests) conditions.\\n</Info>',\n",
       "  'title': 'Classification',\n",
       "  'description': 'Overview of the Classification Quality Preset',\n",
       "  'filename': 'docs-main/metrics/preset_classification.mdx'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use it for search:\n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "v = embedding_model.encode(query) \n",
    "# we feed vector embedding to our index now, not a search string:\n",
    "results = evidently_vindex.search(v)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928c559-665b-452f-8ada-2808a3df2e98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49b3f68f-9e91-4b04-8ebd-6cede1e8c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': 'Retrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.\\n\\nInstead of manually creating test cases, you can generate them directly from your knowledge source, ensuring accurate and relevant ground truth data.\\n\\n## Create a RAG test dataset\\n\\nYou can generate ground truth RAG dataset from your data source.\\n\\n### 1. Create a Project\\n\\nIn the Evidently UI, start a new Project or open an existing one.\\n\\n* Navigate to “Datasets” in the left menu.\\n* Click “Generate” and select the “RAG” option.\\n\\n![](/images/synthetic/synthetic_data_select_method.png)\\n\\n### 2. Upload your knowledge base\\n\\nSelect a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs. Choose how many inputs to generate.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n\\nSimply drop the file, then:\\n\\n* Choose the number of inputs to generate.\\n* Choose if you want to include the context used to generate the answer.\\n\\n![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n\\nThe system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n\\n<Info>\\n  Note that it may take some time to process the dataset. Limits apply on the free plan.\\n</Info>\\n\\n### 3. Review the test cases\\n\\nYou can preview and refine the generated dataset.\\n\\n![](/images/synthetic/synthetic_data_rag_example_result.png)\\n\\nYou can:\\n\\n* Use “More like this” to add more variations.\\n* Drop rows that aren’t relevant.\\n* Manually edit questions or responses.\\n\\n### 4. Save the Dataset\\n\\nOnce you are finished, store the dataset. You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n\\n<Info>\\n  **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n</Info>',\n",
       "  'title': 'RAG evaluation dataset',\n",
       "  'description': 'Synthetic data for RAG.',\n",
       "  'filename': 'docs-main/synthetic-data/rag_data.mdx'},\n",
       " {'start': 3000,\n",
       "  'chunk': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 2000,\n",
       "  'chunk': 'ho painted the Mona Lisa?\", \"Leonardo da Vinci painted the Mona Lisa.\"],\\n    [\"What’s the fastest animal on land?\", \"The cheetah is the fastest land animal, capable of running up to 75 miles per hour.\"],\\n    [\"Can you help me with my math homework?\", \"I\\'m sorry, but I can\\'t assist with homework.\"],\\n    [\"How many states are there in the USA?\", \"USA has 50 states.\"],\\n    [\"What’s the primary function of the heart?\", \"The primary function of the heart is to pump blood throughout the body.\"],\\n    [\"Can you tell me the latest stock market trends?\", \"I\\'m sorry, but I can\\'t provide real-time stock market trends. You might want to check a financial news website or consult a financial advisor.\"]\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n#eval_df.head()\\n```\\n\\n<Info>\\n  **Preparing your own data**. You can provide data with any structure. Some common setups:\\n\\n  - Inputs and outputs from your LLM\\n  - Inputs, outputs, and reference outputs (for comparison)\\n  - Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definiti',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'docs-main/quickstart_llm.mdx'},\n",
       " {'start': 0,\n",
       "  'chunk': \"When working on an AI system, you need test data to run automated evaluations for quality and safety. A test dataset is a structured set of test cases. It can contain:\\n\\n* Just the inputs, or\\n* Both inputs and expected outputs (ground truth).\\n\\nYou can use this test dataset to:\\n\\n* Run **experiments** and track if changes improve or degrade system performance.\\n* Run **regression testing** to ensure updates don’t break what was already working.\\n* **Stress-test** your system with complex or adversarial inputs to check its resilience.\\n\\n![](/images/synthetic/synthetic_experiments_img.png)\\n\\nYou can create test datasets manually, collect them from real or historical data, or generate them synthetically. While real data is best, it is not always available or sufficient to cover all cases. Public LLM benchmarks help with general model comparisons but don’t reflect your specific use case. Manually writing test cases takes time and effort.\\n\\n**Synthetic data helps here**. It’s especially useful when you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. I\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \" you are:\\n\\n* You're starting from scratch and don’t have real data.\\n* You need to scale a manually designed dataset with more variation.\\n* You want to test edge cases, adversarial inputs, or system robustness.\\n* You're evaluating complex AI systems like RAG and AI agents.\\n\\n![](/images/synthetic/synthetic_adversarial_img.png)\\n\\nSynthetic data is not a replacement for real data or expert-designed tests — it’s a way to add variety and speed up the process. With synthetic data you can:\\n\\n* Quickly generate hundreds structured test cases.\\n* Fill gaps by adding missing scenarios and tricky inputs.\\n* Create controlled variations to evaluate specific weaknesses.\\n\\nIt’s a practical way to expand your evaluation dataset efficiently while keeping human expertise focused on high-value testing.\\n\\nSynthetic data can also work for **complex AI systems** where designing test cases is simply difficult. For example, in RAG evaluation, synthetic data helps create input-output datasets from knowledge bases. In AI agent testing, it enables multi-turn interactions across different scenarios.\",\n",
       "  'title': 'Why synthetic data?',\n",
       "  'description': 'When do you need synthetic data in LLM evaluations.',\n",
       "  'filename': 'docs-main/synthetic-data/why_synthetic.mdx'},\n",
       " {'start': 1000,\n",
       "  'chunk': \"a set of Tests to evaluate your data or AI system. Each Report Preset has this option. \\n\\nEnable it by setting `include_tests=True` on the Report level. (Default: False).\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset(),\\n],\\ninclude_tests=True)\\n```\\n\\nFor example, while the `DataSummaryPreset()` Report simply shows descriptive stats of your data, adding the Tests will additionally run multiple checks on data quality and expected column statistics.\\n\\nThe automatic Test conditions can either\\n* be derived from a reference dataset, or\\n* use built-in heuristics.\\n\\n**Using reference**. When you provide a reference dataset, Tests compare the new data against it:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, eval_data_2) # eval_data_2 is reference\\n```\\n\\nFor example, the check on missing values will validate if the current share of missing values is within +/-10% of the reference.\\n\\n<Note>\\nNote that in this case the order matters: the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline and use to generate test conditions.\\n</Note>\\n\\n**Using heuristics**. Without reference, Tests use predefined rules:\\n\\n```Python\\nmy_eval = report.run(eval_data_1, None) # no reference data\\n```\\n\\nIn this case, the missing values Test simply expects 0% missing values. Similarly, classification accuracy Test will compare the performance against a dummy model, etc. Some metrics (like min/max/mean values) don't have default heuristics.\\n\\n<Info>\\n  **How to check Test defaults?** Consult the [All Metrics](/metrics/all_metrics) reference table.\\n</Info>\\n\\n### Individual Tests with defaults\\n\\nPresets are great for a start or quick sanity checks, but often you'd want to select specific Tests. For example, instead of running checks on all value statistics, validate only mean or max.\\n\\nYou can pick the Tests while still using default conditions.\\n\\n**Select Tests**. List the individual Metrics, and choose the the `include_Tests` option:\\n\\n```Py\",\n",
       "  'title': 'Tests',\n",
       "  'description': 'How to run conditional checks.',\n",
       "  'filename': 'docs-main/docs/library/tests.mdx'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text search is fast and efficient. It works well for exact matches and specific terms, and requires no model inference. \n",
    "# However, it misses semantically similar but differently worded queries and struggles to handle synonyms effectively.\n",
    "# Vector search captures semantic meaning and handles paraphrased questions. It works with synonyms and related concepts. \n",
    "# But it may miss exact keyword matches.\n",
    "# Combining both approaches gives us the best of both worlds. This is known as \"hybrid search.\"\n",
    "# The code is quite simple:\n",
    "    \n",
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "\n",
    "text_results = index.search(query, num_results=3)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = evidently_vindex.search(q, num_results=3)\n",
    "\n",
    "final_results = text_results + vector_results\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f6ff4-7aff-410a-ba9a-92459ab86886",
   "metadata": {},
   "source": [
    "## Search done - putting this all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac9b1b3-6ff6-48b3-ad8a-de778420752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our search is implemented!\n",
    "# But before we can use it in our agent, we need to organize the code. \n",
    "# Let's put all the code into different functions:\n",
    "# def text_search(query):\n",
    "#     return faq_index.search(query, num_results=5)\n",
    "\n",
    "# def vector_search(query):\n",
    "#     q = embedding_model.encode(query)\n",
    "#     return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "# def hybrid_search(query):\n",
    "#     text_results = text_search(query)\n",
    "#     vector_results = vector_search(query)\n",
    "    \n",
    "#     # Combine and deduplicate results\n",
    "#     seen_ids = set()\n",
    "#     combined_results = []\n",
    "\n",
    "#     for result in text_results + vector_results:\n",
    "#         if result['filename'] not in seen_ids:\n",
    "#             seen_ids.add(result['filename'])\n",
    "#             combined_results.append(result)\n",
    "    \n",
    "#     return combined_results\n",
    "\n",
    "    \n",
    "# Selecting the best approach\n",
    "# We have seen 3 approaches: text search, vector search, and hybrid search. You may wonder, how do I select the best one? \n",
    "# We will discuss evaluation methods later in the course.\n",
    "# But like with chunking, you should always start with the simplest approach. \n",
    "# For search, that's text search. It's faster, easier to debug, and works well for many use cases. \n",
    "# Only add complexity when a simple text search isn't sufficient.\n",
    "# But let's first build our agent! Our data is ready. \n",
    "# Next, we will build a conversational agent that can answer questions based on the data we collected.\n",
    "# If you have suggestions about the course content or want to improve something, let me know!\n",
    "\n",
    "# TODO:\n",
    "# For the project you selected, index the data\n",
    "# Experiment with text and vector search\n",
    "# Which approach makes sense for your application? Manually inspect the results and analyze what works best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa9f12-2880-4d4b-bab1-d1f2b77be7b5",
   "metadata": {},
   "source": [
    "## Agents and tools - with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc2bdf98-ef05-4d13-b4f5-8e6492283d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first part of the course, we focused on data preparation. Now the data is prepared and indexed so that we can use it for AI agents\n",
    "\n",
    "# So far, we have done:\n",
    "# Day 1: Downloaded the data from a GitHub repository\n",
    "# Day 2: Processed it by chunking it where necessary\n",
    "# Day 3: Indexed the data so it's searchable\n",
    "\n",
    "# Note that it took us quite a lot of time. We're halfway through the course, and only now we started working on agents. \n",
    "# Most of the time so far, we have spent on data preparation.\n",
    "# This is not a coincidence. Data preparation is the most time-consuming and critical part of building AI agents. \n",
    "# Without properly prepared, cleaned, and indexed data, even the most sophisticated agent will provide poor results.\n",
    "# Now it's time to create an AI agent that will use this data through the search engine that we created yesterday.\n",
    "# This allows us to build context-aware agents. They can provide accurate, relevant answers based on your specific domain knowledge rather than just general training data.\n",
    "# In particular, we will:\n",
    "# Learn what makes an AI system \"agentic\" through tool use\n",
    "# Build an agent that can use the search function\n",
    "# Use Pydantic AI to make it easier to implement agents\n",
    "# At the end of this lesson, you'll have a working AI Agent that you can answer your questions in a Jupyter notebook.\n",
    "# 1. Tools and Agents\n",
    "# You can find many agent definitions online.\n",
    "# But we will use a simple one: an agent is an LLM that can not only generate texts, but also invoke tools. Tools are external functions that the LLM can call in order to retrieve information, perform calculations, or take actions.\n",
    "# In our case, the agent needs to answer our questions using the content of the GitHub repository. So, the tool (only one) is a search(query).\n",
    "\n",
    "# Pydantic AI\n",
    "# Dealing with function calls can be cumbersome. We first need to understand which function we need to invoke. Then we need to pass the results back to the LLM and perform other tasks. It's easy to make a mistake there.\n",
    "# That's why we'll use a library to handle it. There are many agentic libraries: OpenAI Agents SDK, Langchain, Pydantic AI, and many more. For educational purposes, I also implemented an agents library. It's called ToyAIKit. We won't use it here, but I often use it in my lessons.\n",
    "# Today, we will use Pydantic AI. I like its API; it's simpler than other libraries and has good documentation.\n",
    "\n",
    "# Let's install it:\n",
    "# uv add pydantic-ai\n",
    "\n",
    "# For Pydantic AI (and for other agents libraries), we don't need to describe the function in the \n",
    "# JSON format like we did with the plain OpenAI API. The libraries take care of it.\n",
    "# But we do need to add docstrings and type hints to our function. I asked ChatGPT to do it:\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e3de46f-9383-44c3-9bcc-d79afafa331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want the agent to make multiple search queries, we can modify the prompt:\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcea1760-495d-4c8d-9a62-a31fe5112178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🔑 Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "# we have to enter OpenAI key here and then we can run all cells freely...\n",
    "import os\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b20567a0-9873-4dbc-8542-fc06470009fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now define an agent with Pydantic AI and give it the text_search tool:\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"evidently_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "604b93bd-8d79-46c1-8036-d4be12c32dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"A test dataset for AI evaluation should include several key components to ensure thorough and effective assessment of the AI's capabilities. Here are the critical elements to include:\\n\\n1. **Diverse Data Samples**: The dataset should contain a wide range of examples that reflect the different scenarios the AI may encounter. This includes variations in topics, styles, and complexities to assess the model's performance across various situations.\\n\\n2. **Ground Truth Labels**: Each sample in the dataset should be annotated with the correct answers or labels. This ground truth is essential for comparison to determine how accurately the AI's outputs match the expected results.\\n\\n3. **Contextual Information**: For tasks that involve retrieval or generation of context-based information (e.g., Retrieval-Augmented Generation systems), it's important to include the relevant context or background materials that the AI can refer to for producing accurate responses.\\n\\n4. **Evaluation Metrics**: The dataset should also incorporate criteria for evaluating the AI's outputs. This could include qualitative measures (like sentiment analysis) and quantitative metrics (like accuracy, precision, and recall).\\n\\n5. **Inputs and Outputs**: The dataset should clearly define the expected inputs and corresponding outputs. This structure facilitates a straightforward comparison and allows for identifying where the AI's performance may fall short.\\n\\n6. **Variability and Complexity**: It’s beneficial to include examples that test the limits of the model, such as ambiguous questions, edge cases, or inputs that are likely to produce difficult or unexpected outputs.\\n\\n7. **Realistic Use Cases**: The scenarios presented in the dataset should closely mimic actual use cases the AI will face in production to yield meaningful evaluation results.\\n\\nBy combining these elements, a test dataset can effectively facilitate the evaluation of an AI system's performance and guide improvements.\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't need to do anything with our text_search function. We just pass it directly to the agent.\n",
    "# Let's run it:\n",
    "question = 'What should be in a test dataset for AI evaluation?'\n",
    "\n",
    "result = await agent.run(user_prompt=question)\n",
    "result\n",
    "\n",
    "# We use await because Pydantic AI is asynchronous. If you're not running in Jupyter, you need to use asyncio.run():\n",
    "# import asyncio\n",
    "\n",
    "# result = asyncio.run(agent.run(user_prompt=question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08b313a4-4e8d-416f-87f2-1d9e7258ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look inside the result to get a detailed breakdown of the agent's reasoning and actions:\n",
    "# result.new_messages()\n",
    "\n",
    "# Pydantic AI and other frameworks handle all the complexity of function calling for us. We don't need to manually parse responses, \n",
    "# handle tool calls, or manage conversation history. This makes our code cleaner and less error-prone.\n",
    "# We implemented an agent. Great! But how good is it? Is the prompt we came up good? What's better for our agent, \n",
    "# text search, vector search or hybrid? Tomorrow we will be able to answer these questions: \n",
    "# we will learn how to use AI to evaluate our agent.\n",
    "# TODO\n",
    "# Do the same, but for the documentation we extracted instead of FAQ\n",
    "# For the project you selected, create an agent with Pydantic AI\n",
    "# Interact with the agent, improve the system prompt if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e0c7b-71cf-4e69-a7cc-4d5c3cc3e01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
